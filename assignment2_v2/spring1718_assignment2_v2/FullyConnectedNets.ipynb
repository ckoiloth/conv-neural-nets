{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Neural Nets\n",
    "In the previous homework you implemented a fully-connected two-layer neural network on CIFAR-10. The implementation was simple but not very modular since the loss and gradient were computed in a single monolithic function. This is manageable for a simple two-layer network, but would become impractical as we move to bigger models. Ideally we want to build networks using a more modular design so that we can implement different layer types in isolation and then snap them together into models with different architectures.\n",
    "\n",
    "In this exercise we will implement fully-connected networks using a more modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
    "\n",
    "In addition to implementing fully-connected networks of arbitrary depth, we will also explore different update rules for optimization, and introduce Dropout as a regularizer and Batch/Layer Normalization as a tool to more efficiently optimize deep networks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train: ', (49000, 3, 32, 32))\n",
      "('y_train: ', (49000,))\n",
      "('X_val: ', (1000, 3, 32, 32))\n",
      "('y_val: ', (1000,))\n",
      "('X_test: ', (1000, 3, 32, 32))\n",
      "('y_test: ', (1000,))\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "  print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: foward\n",
    "Open the file `cs231n/layers.py` and implement the `affine_forward` function.\n",
    "\n",
    "Once you are done you can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.769849468192957e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: backward\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  5.399100368651805e-11\n",
      "dw error:  9.904211865398145e-11\n",
      "db error:  2.4122867568119087e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU activation: forward\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.999999798022158e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU activation: backward\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.2756349136310288e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1: \n",
    "\n",
    "We've only asked you to implement ReLU, but there are a number of different activation functions that one could use in neural networks, each with its pros and cons. In particular, an issue commonly seen with activation functions is getting zero (or close to zero) gradient flow during backpropagation. Which of the following activation functions have this problem? If you consider these functions in the one dimensional case, what types of input would lead to this behaviour?\n",
    "1. Sigmoid\n",
    "2. ReLU\n",
    "3. Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "Sigmoid and the ReLU function have this problem. For the sigmoid function as, the derivitive will be get closer to zero the bigger the magnitude of the input. More clearly, around 4 and -4 is where the derivitive starts becoming 0. For the ReLU function, the derivitive is zero if the input is negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `cs231n/layer_utils.py`.\n",
    "\n",
    "For now take a look at the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_relu_forward and affine_relu_backward:\n",
      "dx error:  2.299579177309368e-11\n",
      "dw error:  8.162011105764925e-11\n",
      "db error:  7.826724021458994e-12\n"
     ]
    }
   ],
   "source": [
    "from cs231n.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# Relative error should be around e-10 or less\n",
    "print('Testing affine_relu_forward and affine_relu_backward:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss layers: Softmax and SVM\n",
    "You implemented these loss functions in the last assignment, so we'll give them to you for free here. You should still make sure you understand how they work by looking at the implementations in `cs231n/layers.py`.\n",
    "\n",
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing svm_loss:\n",
      "loss:  8.999602749096233\n",
      "dx error:  1.4021566006651672e-09\n",
      "\n",
      "Testing softmax_loss:\n",
      "loss:  2.302545844500738\n",
      "dx error:  9.384673161989355e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "\n",
    "# Test svm_loss function. Loss should be around 9 and dx error should be around the order of e-9\n",
    "print('Testing svm_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer network\n",
    "In the previous assignment you implemented a two-layer neural network in a single monolithic class. Now that you have implemented modular versions of the necessary layers, you will reimplement the two layer network using these modular implementations.\n",
    "\n",
    "Open the file `cs231n/classifiers/fc_net.py` and complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization)\n",
      "Running numeric gradient check with reg =  0.0\n",
      "W1 relative error: 1.83e-08\n",
      "W2 relative error: 3.12e-10\n",
      "b1 relative error: 9.83e-09\n",
      "b2 relative error: 4.33e-10\n",
      "Running numeric gradient check with reg =  0.7\n",
      "W1 relative error: 2.53e-07\n",
      "W2 relative error: 2.85e-08\n",
      "b1 relative error: 1.56e-08\n",
      "b2 relative error: 7.76e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "# Errors should be around e-7 or less\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "In the previous assignment, the logic for training models was coupled to the models themselves. Following a more modular design, for this assignment we have split the logic for training models into a separate class.\n",
    "\n",
    "Open the file `cs231n/solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves at least `50%` accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.514\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(reg = 1)\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
    "# 50% accuracy on the validation set.                                        #\n",
    "##############################################################################\n",
    "solver = Solver(model, data, optim_config = {\"learning_rate\" : 1e-3} , lr_decay = 0.94, verbose = False);\n",
    "solver.train();\n",
    "print(solver.val_acc_history[-1])\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAALJCAYAAAAnCMuGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3X+Um9d5J/bvBeYliaFsgrTproiQ\nolabJROW4oxFR0rY04bqOWJiWdqJZJtR5LSb09S7bbonZHTmhMoqFuXVrthOHbHZnK7X++Nsc6RV\nRxKVqWQ6pbqHapPQoWzSMxTDWEwiS6IMKTFjcmSbA5EY4PYP4IIvXtx73/v+Al7MfD/nOBFnMMDF\nixfve5/743mElBJERERERESUT4VBN4CIiIiIiIjMGLQRERERERHlGIM2IiIiIiKiHGPQRkRERERE\nlGMM2oiIiIiIiHKMQRsREREREVGOMWgjIqKhIoQoCiF+JITYlOZjY7TjcSHEf0j7eYmIiIJGBt0A\nIiJa2oQQP/L9cxTAVQCN9r//kZTy6SjPJ6VsALgh7ccSERHlFYM2IiLKlJSyEzQJId4C8KtSyv9k\nerwQYkRKudiPthEREQ0DLo8kIqKBai8znBZCPCOE+CGAzwkhfloIcVIIMS+EeE8I8btCCK/9+BEh\nhBRCbG7/+6n27/9QCPFDIcSfCiFujvrY9u9/XgjxF0KI94UQ/1IIcUII8Q8d38eEEOJcu83HhRBb\nfL/7LSHEu0KIHwghXhdC/Gz753cIIb7V/vnfCCGmUjikRES0xDBoIyKiPPgFAP8RwBoA0wAWAfw6\ngI8C2AXg5wD8I8vf/xKA3wawDsAFAP8s6mOFEB8D8CyAyfbrvgngp1waL4T4CQBPAfgnANYD+E8A\nXhJCeEKIbe22f1xK+WEAP99+XQD4lwCm2j//ewCed3k9IiJaXhi0ERFRHvyJlPIlKWVTSlmTUn5T\nSvmqlHJRSvkdAF8B8F9Z/v55KeUpKWUdwNMAxmI89lMA5qSU/1f7d08C+FvH9v8igBellMfbf3sI\nwIcB3I5WALoKwLb20s832+8JAOoAflwI8REp5Q+llK86vh4RES0jDNqIiCgP3vH/QwixVQhxVAjx\n10KIHwD4IlqzXyZ/7fvvBdiTj5geu8HfDimlBPBdh7arv33b97fN9t9WpJTnATyE1nv4XnsZ6N9p\nP/RXAPwkgPNCiG8IIT7p+HpERLSMMGgjIqI8kIF//2sAfwbg77WXDn4BgMi4De8B+DH1DyGEAFBx\n/Nt3Adzk+9tC+7mqACClfEpKuQvAzQCKAJ5o//y8lPIXAXwMwJcAHBFCrEr+VoiIaClh0EZERHn0\nIQDvA7jS3i9m28+Wlq8C+LgQ4h4hxAhae+rWO/7tswDuFUL8bDthyiSAHwJ4VQjxE0KI3UKIlQBq\n7f81AEAI8ctCiI+2Z+beRyt4bab7toiIaNgxaCMiojx6CMB/i1bg86/RSk6SKSnl3wDYC+B3AHwf\nwC0AZtGqKxf2t+fQau+/AnARrcQp97b3t60E8L+gtT/urwGsBfBI+08/CeDb7ayZ/yuAvVLKaym+\nLSIiWgJEa8k+ERER+Qkhimgte/y0lPKPB90eIiJavjjTRkRE1CaE+DkhxJr2UsbfRivz4zcG3Cwi\nIlrmGLQRERFd918A+A5aSxl/DsCElDJ0eSQREVGWuDySiIiIiIgoxzjTRkRERERElGMjg3rhj370\no3Lz5s2DenkiIiIiIqKBOn369N9KKUPLywwsaNu8eTNOnTo1qJcnIiIiIiIaKCHE2y6P4/JIIiIi\nIiKiHGPQRkRERERElGMM2oiIiIiIiHKMQRsREREREVGOMWgjIiIiIiLKMQZtREREREREOcagjYiI\niIiIKMcYtBEREREREeUYgzYiIiIiIqIcGxl0A/LikZmzeObVd9CQEkUh8MDtG/H4xPZBN4uIiIiI\niJY5zrShFbA9dfICGlICABpS4qmTF/Dgv/nTAbeMiIiIiIiWOwZtAJ559R3tz0+8cQkzs9U+t4aI\niIiIiOg6Bm1AZ4ZN5+CL5/rYEiIiIiIiom4M2gAUhTD+br5W72NLiIiIiIiIujFoA/DA7RsH3QQi\nIiIiIiItBm0As0QSEREREVFuMWgjIiIiIiLKMQZtREREREREOcagjYiIiIiIKMcYtBEREREREeUY\ngzYiIiIiIqIcY9DWtnpF0fi7mdlqH1tCRERERER0XWjQJoTYKIR4RQjxbSHEOSHEr1se+wkhREMI\n8el0m5m9f/4L5rT/B18818eWEBERERERXecy07YI4CEp5U8AuAPArwkhfjL4ICFEEcD/DOBYuk3s\nj4nxivF387V6H1tCRERERER0XWjQJqV8T0r5rfZ//xDAtwHoIpx/AuAIgO+l2kIiIiIiIqJlLNKe\nNiHEZgDjAF4N/LwC4BcAfDnk7z8vhDglhDh18eLFaC3tg7WjXqSfExERERERZc05aBNC3IDWTNo+\nKeUPAr8+DOA3pZQN23NIKb8ipdwppdy5fv366K3N2KP3bINXFF0/84oCj96zbUAtIiIiIiKi5W7E\n5UFCCA+tgO1pKeULmofsBPB/CiEA4KMAPimEWJRSzqTW0j6YGK/g1NuX8Myr76AhJYpCYO8nNlr3\nuxEREREREWXJJXukAPDvAHxbSvk7usdIKW+WUm6WUm4G8DyA/3HYAjagldr/yOkqGlICABpS4sjp\nKlP+ExERERHRwLgsj9wF4JcB3CmEmGv/75NCiH8shPjHGbevr6aOnUet3r3Cs1ZvYOrY+QG1iIiI\niIiIlrvQ5ZFSyj8BIMIe53v8P0zSoEF6d74W6edERERERERZi5Q9cqkrG7JEmn5ORERERESUNQZt\nPu2tbD3mF+rc10ZERERERAPBoM1nvlbX/lwCmHz+DAM3IiIiIiLqOwZtPsKyc6/ekExIQkRERERE\nfcegzce0PFJhQhIiIiIiIuo3Bm0RbCiXBt0EIiIiIiJaZhi0+ZRL5iyRXlFgcs+WPraGiIiIiIiI\nQVuXg/dug1fQb2zb+4mNmBiv9LlFRERERES03DFo85kYr+Cnbl6r/d0z33iH2SOJiIiIiKjvGLQF\nnPzOZe3PG01mjyQiIiIiov5j0BbQsKSQZPZIIiIiIiLqNwZtETB7JBERERER9RuDtgiYPZKIiIiI\niPqNQVsEzB5JRERERET9xqAtYO2ovlab6edERERERERZYtAW8Og92+AVe2u13X3rjQNoDRERERER\nLXcM2gImxivY+4mNCIZtT528gPEvvsxabURERERE1FcM2jSOvvYedIn/Ly/UsX96Do/MnO17m4iI\niIiIaHli0BYwM1vF5YW68fcSwNMnL3DGjYiIiIiI+oJBW8DUsfOhj5GOjyMiIiIiIkqKQVvAu/O1\nVB9HRERERESUBIO2gA3lUqqPIyIiIiIiSoJBW8Dkni09mSODRPtxREREREREWWPQFjAxXsHP3LLO\n+pgH79iEifFKn1pERERERETL2cigG5BHb31fv19NCODJz44xYCMiIiIior7hTJuGKcmIlGDARkRE\nREREfcWgTcOUZEQArM9GRERERER9xaBNw5RkhPXZiIiIiIio3xi0RVSdr2HXoeOccSMiIiIior5g\n0Kbx2EvnrL+vztfw8AtnGbgREREREVHmmD1S4/JCPfQxtXoDU8fOdxKTzMxWMXXsPN6dr2FDuYTJ\nPVuYtISIiIiIiBJj0JaAyjI5M1vFwy+cRa3eAHB9Jg5gtkkiIiIiIkqGyyM1yiXP6XEqy+TUsfOd\ngE1RM3FERERERERJMGjTOHjvNngFYX1MySt2skya6rqZfk5EREREROSKQZvGxHgFU5/ZgUq5BAGg\nUi7hc3ds6vr3E/dt7yx9NNV1M/2ciIiIiIjIVeieNiHERgC/D+DvAGgC+IqU8n8LPOZBAL/Z/ueP\nAPwPUsozKbe1r1RAppKLvPL6RWNykck9W7r2tAHdM3FERERERERxCSml/QFC3AjgRinlt4QQHwJw\nGsCElPLPfY/5GQDfllJeFkL8PICDUsrbbc+7c+dOeerUqeTvICPB5CIAINAqsF3RZIdk9kgiIiIi\nIopCCHFaSrkz7HGhM21SyvcAvNf+7x8KIb4NoALgz32P+brvT04C+LHILc4ZXXIRFd7qskNOjFcY\npBERERERUeoi7WkTQmwGMA7gVcvD/jsAf2j4+88LIU4JIU5dvHgxykv3XVgSEWaHJCIiIiKifnAO\n2oQQNwA4AmCflPIHhsfsRito+03d76WUX5FS7pRS7ly/fn2c9vZNeTQ87X+V2SGJiIiIiChjTkGb\nEMJDK2B7Wkr5guExtwL4twD+gZTy++k1cTBCtvoBaO1xm5mtZt4WIiIiIiJavkKDNiGEAPDv0Eo0\n8juGx2wC8AKAX5ZS/kW6TRyM92v10MdIgEskiYiIiIgoU6GJSADsAvDLAM4KIebaP/stAJsAQEr5\nZQBfAPARAP97K8bDoksWlDzbUC45LX9kAW0iIiIiIsqSS/bIP0FrJaDtMb8K4FfTalQeTO7Zgn3T\nc6GPYwFtIiIiIiLKUqTskcuJS/p+FtAmIiIiIqKsuSyPXJYemTlr/X255EEIYP/0HKaOne8qpj0z\nW8XBF89hvr0vbu2oh0fv2cY6bkREREREFBmDNoNnXn3H+Ltdt6zDty683ym+7S+2DQCTz51BvXk9\n/eTlhTomnz8DwG0Gj4iIiIiISGHQZtCw5Pz/0+9cQjPwa3+x7XrwlwDqDYmpY+cZtBERERERUSQM\n2gyKQhgDN01MBiA8kyQzTRIRERERUVRMRGLwwO0bI//NhnLJmk2SmSaJiIiIiCgqBm0Gj09st9c5\nCFCZJCf3bIFX6P1LryiYaZKIiIiIiCLj8kgL8662bkUh8MR927v2qzF7JBERERERpYFBm0WlXEI1\nZB+aQGsppT8gmxivMEAjIiIiIqJUcHmkxeSeLSh5RetjJIAjp6uYma32p1FERERERLSsMGizmBiv\n4In7tqNc8qyP86f7JyIiIiIiShODNgdXF5uhj2E6fyIiIiIiygKDthBTx86jVm+EPo7p/ImIiIiI\nKAsM2kK4zKAJgOn8iYiIiIgoEwzaQqwJ2c8GtJKRMFskERERERFlgUFbCOFQYbvCpZFERERERJQR\nBm0h5hfq1t+XvCKXRhIRERERUWYYtIUoj5qXRxaFwBP3befSSCIiIiIiyszIoBuQd1Kaf9eQslOf\nLRi4zcxWMXXsPN6dr2FDuYTJPVsY3BERERERUWQM2kK8X7Mvj6zO1/DwC2c7/546dh7V+RoEWglK\ngo9h4EZERERERFEwaAuxoVxCNSTtf63ewL7pua6fBSfoavUGpo6dZ9BGRERERESRcE9biMk9W+CQ\nQNKJS803IiIiIiIiPwZtISbGKz2zZnFtYGkAIiIiIiKKiEGbg7JDge0wLA1ARERERERxcE+bA5cC\n2zYVZo8kIiIiIqKYGLQ5CCuwbVIueZh79K6UW0NERERERMsJl0c6iLMXTQD41I4b028MEREREREt\nKwzaHEzu2YKSV4z0NxLAkdNVzMxWs2kUEREREREtCwzaHEyMV/DEfdsj/52qzUZERERERBQXgzZH\nE+OVWFkkWZuNiIiIiIiSYNAWwcF7t0X+G9f9cDOzVew6dBw3HziKXYeOc1klEREREREBYPZIJzOz\nVUwdO49qxFkzryCcarPNzFbx8AtnUas3AADV+RoefuEsALBMABERERHRMsegLUQwoIrEsb7b1LHz\nPc+v9sMtp6BNBcfvztewgbXtiIiIiIgAcHlkKF1A5arekE6JSEz73pbTfjgVHFfna5C4PtvIZaJE\nREREtNwxaAvhEjgVhXlKzeXvy6P6BCdx6sMNK9tsIxERERHRchYatAkhNgohXhFCfFsIcU4I8eua\nxwghxO8KIf5KCPGaEOLj2TS3/1wCp4aUxt8VhLDOFs3MVvGjDxZ7fu4V3fbDLRWcbSQiIiIi0nOZ\naVsE8JCU8icA3AHg14QQPxl4zM8D+PH2/z4P4F+l2soBilNY268hpXWZ39Sx86g3e4O+1StGltV+\nLlNwvJxmG4mIiIiIdEKDNinle1LKb7X/+4cAvg0gGE38AwC/L1tOAigLIW5MvbUDoApr25ZAhrEt\n8zPNJL1fq/f8bCmXBdAFxyWvuKxmG4mIiIiIdCJljxRCbAYwDuDVwK8qAN7x/fu77Z+9l6BtuaFm\nvGJnkYQ5ONtQLmlLCQRnmJKWBch7ZkbVljy3kYiIiIhoEJyDNiHEDQCOANgnpfxB8NeaP+lZ8yeE\n+DxayyexadOmCM0cPH9QEbVeGwCsKemTjUzu2dITDOpmmJKUBRiWOnAT45VctYeIiIiIKA+cskcK\nITy0AranpZQvaB7yXQAbff/+MQDvBh8kpfyKlHKnlHLn+vXr47R3oCbGK7H3uF25toiZ2WrPEkcA\neOK+7aiUSxAAKuUS7r+tgqlj57uWQSZJ1MHMjEREREREw0tIS+ZDoJUZEsD/AeCSlHKf4TF3A/if\nAHwSwO0AfldK+VO25925c6c8depUrEYPglpeGGeWTRHonX70igJTn97RmWHSFfMueUWs8gq4vNC7\nz63SXkZoW1Z484GjvdOe7fa8eeju2O+HiIiIiIjiE0KcllLuDHucy/LIXQB+GcBZIcRc+2e/BWAT\nAEgpvwzga2gFbH8FYAHAr8RpdF7pAqk4dIFTvSHx2EvnupZf6mbFVo4UUPKKPcHc7q3rQ5c+uu6b\ns8n7njgiIiIioqXKJXvkn0gphZTyVinlWPt/X5NSfrkdsKGdNfLXpJS3SCm3SymHZwrNgS6QSpN/\nBs2WTTK4jPKJ+7bjldcvhi59TJqZUQWt1fkaJK4HhkspeyURERERUV5Fyh65XPWjwPOuQ8cxuWeL\ndVZMl6hj//Rcz2OB7jZHzcwYnFVbuLYYOwkKERERERElw6DNgSmQSpOavfr4pjXa19q9VZ+4xXXp\no2tmRl2mSZN+BLNERERERMudU/bI5W5yzxZ4hfjFtV3V6g2c/M5l7e9eef2i9ue6pY9eQeDylavY\nfOAoNh84ivEvvuy8lDHKUtAoe+KIiIiIiCgeBm0OJsYruGFVfyYlG4ZsnqZZrYnxStdet3LJQ0NK\nLNSbncdcXqhj8vkzToGb6+xZlD1xREtJsGwH93YSERFR1hi0OZrXpNv3y3oezjarNTFewYkDd+LN\nQ3dj9coRNDVxX70hneqymV6nXPJ6kqBwPxstN0zKQ0RERIPAPW2Owva12avd9SqXPMzX9IGgrp7b\nQrs4d1igZJspc5lFm9yzRVsn7uC92xikWbAkwvJgK1TPz5uIiIiywpk2BzOzVVy5upja81XKJRy8\nd5vx9xKtoM7v8kI9dER/ZraKgjDP+bnsQQsut+SsWjjOviwfpoEPJuUhIiKiLHGmLURahbX9dm9d\nb12qWBQCutirVm/g4IvntAGUaqdpT5xXFM570FwzTcYVd1Yqr7NZnH1ZPtIoVE9EREQUFWfaQmRR\nWPurZ96zjsw3pOwquO03X6trZ3Bs7Vw76mHq0ztyEUDEnZXK82wWZ1+Wj6SF6omIiIjiYNAWIouO\n93ytbl3GGEY3S2dqpwAw+4W7IgdsWWXIs81KZfF3/WCaZeHsy9LD5cNEREQ0CFweGSKrwtqmZYwu\ndAFamsu2dAW2H37hLAAk6pzOzFaNxzIsOM7zbJYpeQtnX5amrJcPExEREQVxpi2EbjmUqwSTaVbB\nQMyUKCUscDDNpmUxq6UCQZOw4DLPs1mcfSEiIiKiLHGmLYTqePsTYMwvXMOVa+H73BJMphkFE4qY\nEqWsHfXw6D3mNP222bQsZrVse+5cZqXyPpuVxuxLXhOtLFf8PIiIiCgvhMwisnCwc+dOeerUqYG8\ndlJZZJR05RUEpj6zA0ArEDItN1QlA1QtuGAQt+vQce3fVtozV6bfnThwZ9fPXDu2Nx84aqxld3jv\n2FBnj0yD7pwqeUXO2A0IPw8iIiLqByHEaSnlzrDHcaYthonxCk69fQnPvPpOor1pcdSbEgdfPIer\ni01r0Bgs3H15oY7J588AaLXfNpv25N4xp1mtKHvfTHvuKuWScyd4Ke8lYtmAfOHn0W0pD5gQEREN\nA+5pi2Fmtoojp6t9D9iU+Vo91ixfvSGxb3oOj8ycte4Rc92jFWXvW79SpWeV9TJreU60shzx87gu\nz+U2iIiIlgvOtMWQRe22fnrq5AXsumUdLl25ZpxNc5nVitKx1e0NTHu0Psusl1nPMrBoc77w87iO\ns45EREsPV1AMHwZtMSyF0favv3EJT+4d035hXb/I5VFPWwS8POp1/rufF4UknUtTO7MKBIPynmhl\nueHncR1nHYmIlpZ+9W0oXQzaYsiqdls/qYWdusQiwS/y5HNn8NhL5zC/UO8KaEyrQ9XP+31RiNu5\ntLWzX7MM/ZiJJHf8PK7jrCMR0dLCFRTDiUFbDLpR+GEU/HLOzFbx0LNnevbq1ZuyM6PmD2jer/XO\nsvl/3u+LQtzOpamdumOhZDHLsJQTrQwjfh4tS3XWkUuDhhs/P6L4uIJiODERSQwT4xXcf1sFxXb1\nbAFgRTGjStoZ8n851WyTS3IVFXiFFbzu90UhbrITU3tsx4KzDLRcLMXi8UyuMtz4+RElE9Z/o3zi\nTFsMweyREq3MjMOmIARmZquYGK9ETq7iUhqg38uq4i5pi7rc1VT+YDmM+i6X90ndos465v084dKg\n4cbPjyiZpbqCYqlj0BaD7oYxfCFbayZJLXWMukdPonUc7r+tgldev6jtnKVxUYja+QvrXOqeL8py\n14qmDXnd0Jt2xzmv75PyZRjOEy4NGm78/IiS4b7t4cSgLYaldGNQo5NFIYzLAb2i0M4kVudrOHK6\nalwqlfSikLTzFwxadm9djyOnqz3P98R92/HEfds7jy0YjkWlXOpJ3KLeXx5Gff3vd03Jw5Vri53P\nLY2Oc17eJw2ebUBgGM4TJlcZbvz8iJLjvu3hw6AtBtMNQ2A4Z9zebe8LMJn69A5MHTuvfc+6zliS\nGR7/3+qCJ//r2V5HF/A9ffJCz/tUz3fiwJ3GvwXsM4R5GPUNtnlekyQmacc5D++TBi9sMGUYzhMu\nDRpu/fj88r7El4iWHyYiicGU8OLBOzZ1kpMMkw3lEiqGEcpKuYSJ8QpOHLgTpnemS2gSZ4N48G9t\nmRvDXifKEtZgZzJq4oU8bOh13ZOYpOOch/dpMzNbxa5Dx3HzgaPYdeg4kxJkxDaTBuT/PAGWZnKV\n5STrz4+JTogojzjTFoNt2d/Om9Zh3/TcgFvoruQVsXvrenz1zHs9v/MKomvk0mVJii19/v7pOeuI\npWvgsaFcCl2CFSU40XUmoywbyMOovev7TdJxzsP7NBmGfVRLRdhMWp7PEz8uDRpuWX5+w7DEl4iW\nHwZtMeluGGo5Rd6pZZwVzT6vngf66DpjAq0O8q5Dx7F763pjQhM1a+bvTAPoLLu07anzU50/U2Cs\nOo6uS1jT6EyG7d3rxzIblwyYSd9rnjcuD0snayksuQobvMnzeTJoS+HzXw6GYYkvLR28LpArBm0p\nmJmt4uCL57T7iPJIBWwnDtyJXYeOG2e36g3Z1en1d8aq87WuAKg6X8NTJy84vX6t3sDBF8/h6mKz\n89q2gM3/Oqu8Ak69fcm4f1B1HE2j/bZsl3FF3VsXdwbI9jq69+sVBG5YNYL5hXpq7zXL0e0kN65h\n6GTlfTbQ9fi7zKQt9VmsOOdq3j9/uo6JTqhfeF2gKBi0JaRLWjEMqvM13HzgaGjiFN1+r4nxCnYd\nOh65TIBflADX38bLC3VtQhGgFdypjmOS0f4oHbKwC26SGaAo2SDTnt3o98hf0htXGp2srN9znmcD\noxz/5T6TFvdczfPnT92GZYkvpWOQM128LlAUDNoSilqUOk9cMl0WhMDNB45ilVfA1cUmmhLOSxmz\nYnplCWD/9Bymjp3vXHTjzGZF6ZDF3VsXNgMUJxtkWrMbj8yc7QqM+zHyl/TGlbST1Y/RzjzPBkY9\n/knOtWFfChT3XM3z50/dlvvAxHIy6Jku0+A3rwukw6AtgZnZaqLZpmGggrNavdnzMxcCwIihzlsW\n/Jm+gOj13MLKDASFdcTKox4uL/QGXGEzQFlmgwxbzmkrjZDVTSxphzZpJytKRzxu0JHnJVf9CigG\n3UFKQ9xjlefPn3oNconvsA9sDJNBznTNzFZDt3oQ+TFoi0l1Pmz6Xbdt9YoirlzL16yfbP+fggCa\nKR6MsGMbZQni5PNnOkGlrcyAjq0jNjNbxY8+WOz5nVcUoTNASbJBmm74ur2XuuWcrqUR0pRGhzZJ\nJ8u1I24LOoDexDoV3/HP65KrmdmqsaB82h2HpbAUKO656vL5D0NnfRjaOMyWwsDGMBnkDLjpfuvf\n6kHkxzptMbnMhJS8Arxi/+q2XVtshj9oAOpNGRqwqfp2lXIJa0c962NVTTxVo8fEZQni/mfnnGYB\nTR0yU82+yT1bMHXsPOqaN756xUjozdels6zr8JvqCz0ycxYPv3DWuswSMC/VcG1TXLbj2A+utcVM\nQcfBF891jjvQmy11Zraay9pg6nzRBWxZHP+lsEQw7rka9vkPQ22wYWjjsAurg6jDGpXxDbKupOm6\nJ8EAnfQ40xaTSydjod7sa1Rcb0oIAfRru1kaM4kqiyVwfQRXt5zQryCAp09ewIZyCU/uHevMbvQ+\nTuCRmbPabJGq8+FyrGwdMtuyvP2GsgTvOyRhiZsN0nTDf+bVd6zLWtX5bNuvmGUANeg9JK6zYKbv\nvS2xjr/DlbcZCtvgk7/dUdppm4lZCksEk5yrttngYZiFHIY2DruoAxucmUtmkCsgTNfDSorXQ86M\nLy0M2mJyqYsFAP2e+5Ky1bnXzfCk7cE7NuHoa++FBllAa9YREMYLY5QsnGoJqLo53X9bRVtrriFl\nVxkC/83MZaZUAE4XOVPNviRLzuJ2DE039rB9iKpNtsdlfaEf5B4S9br+5aOrvN4hF9fvfZA69/LW\nsQobfIqzP9T2PvO6RDSqLM7VYZiFHIY2DruoAxvLLZBOOwgJ3mvXlDwI0ZvULIu2ZX09ZEC/9IRO\nBAkh/r0Q4ntCiD8z/H6NEOI3VFY8AAAgAElEQVQlIcQZIcQ5IcSvpN/M/NEtkcmLqc/sQLlkX2KY\n1OoVxVbCCgmnJaCLTYn7b6sYlwbFzcJZqzfwyusX8cR92ztLLMMery6qNuWShzcP3d1Z5hhl2Ula\nS84mxis4ceBOvHnobpw4cKdzsgsd27EpeUXs3roeuw4dNz4mzZG/PLvqW2J8eaHes/TLtDQubElv\nUYjIS576wWUAIUo7w5Z25XGJaF64LNMa9DK4QS4lWy6iLr9dToF0Vstz1b32yb1juLrYxOWFeuTn\nj9O2rK+HcZbaUr65zLT9BwC/B+D3Db//NQB/LqW8RwixHsB5IcTTUsprKbUxl3QzIQvXFp1mnVys\nHCl0dSCjmnv0Lmw+cDSVtuio2a75Wh1eITxYqjckXnn9YmcppKJGppJk4Xx3voaJ8Qr2GZYjBoW9\nllcQOHjvtlTrMQGtjnvWHVRbUXHdbOTaUQ9333qj9nf+vx+2mZA4XEasTTOgAIwzxSWvaDy2Lh2r\nqKO3UR6vO1/ittP2OP/Ps5xRHbalQMFajF4g065tNULwetSP975UZkrzLOoqi6Ww5NhV1rOKSZ4/\n7t9meT1cTgH9chEatEkp/0gIsdn2EAAfEkIIADcAuASgN2XeEhT8sqVZaDtJwLZveg4HXzyXuA2u\n6k3pVLsteGMJZm6MS9WSS8PaUQ+P3rMNE+OtAuJp1mNqStmXJYZA71KPp09ewCqv0MniWRQCD9y+\nEY9PbNe+T8V/PJY61xtc2L4kXfZI08BEWMcq6sBB1McHz5ekWSQH2YEctqVAulqMXkFg7ain3bca\nNmrej/c+6L2ny0WUjvxyCqTTCEJsgxtJnj+PAdJyCuiXizT2tP0egBcBvAvgQwD2Sim1EYcQ4vMA\nPg8AmzZtSuGl80XXYf7BB/VUU927siVGyEJDytDEJP4lejOzVeyfnkulJEIahb4rms5HnA3hB188\nZ3xPWV4odTcioLsjF6y1d+R0FTtvWme9qXxQz2dG0iwkvcGFdbTidKyijt7GGe31t1s38BSlAzjI\nDuSw7e3RtbfelBhdMYLZL9zV83jb9aif732Qe0+p13IKpJNeo8MGdpI8fx4DpOUU0C8XaQRtewDM\nAbgTwC0A/h8hxB9LKX8QfKCU8isAvgIAO3fuHEAok73gDW38iy+ntmQyzyrlEnZvXd+V+COoISVu\nPnC0E8yGBXhqpmJ+4Vqi+nOiHU2aXk8AOHHgzs5+EXXjMxXGVjN7/pvjIzNnre89jQulKTAz1V5b\n5RWss76qU2dLrhGn4zdsS9SULG9waSeW8f/cf7yT1thL2gEcZAcyjyPdNlHba+sUxn3vefyu5rFN\nebdcAumk1+iwwY0kz5/HAGk5BfTLRRpB268AOCSllAD+SgjxJoCtAL6RwnMPvfllELB5BYGFa4t4\n2hK0KBJus4BvPPHJzn8nXfo4UhCY+vQO4xK1VV4BY4+93BP4eAXRs8cE6K3BdertS6Hv/f7bkt1U\ndSOEk8+dAQS0y0tr9YbTMt1352t4cu+YdVlvdb7WE6RGaWeel6j5ZX2Di9OxChu9dV2S7X982PtL\n2gEcVAcyjyPdNlHba+sUxll+m8fvah7bRPmR9BodNriRtJxHkrZlZbkE9MtFGkHbBQD/NYA/FkL8\nZwC2APhOCs+7JMRNET4sRr0C6k2Z6mxiySs4zXi5qjdkJ3Xv5HNnesoh1OrNrqWDnb9rSpRLHlav\nHDHu96nVG3j61QuhyzzVUkTXtOnBC79pKVVSG8qlTpseevaMcampPxsWYO5A2YpPL9UbR5YzA6Z6\nfQvXFnHzgaPGPWh+qmOfdYfYdhyiHKO4xzPKSHceZnOijszrOoW7t67vBGzB5elho/yDWk5qO/bD\ntsR1KXL5bgzy+xMWhCStE5kkyGGARFkTMuSGL4R4BsDPAvgogL8B8CgADwCklF8WQmxAK8PkjWit\nNDskpXwq7IV37twpT506laTtuRO8WOzeuh5fPfNe3/eX9ZNLApLIz1kQaPgCEq8gjDNKUVRiBNAC\nwJuH7gbQmvFL0gJ/IXEdtScueL7Ysg/alEseri42rVkh/dksXWdtbO/DdowO7x3LxQ3NdFM37eey\nZfyM8zdJ2rum5OHKtUWn70KwzuCuQ8e1539RCDSlTNT5sh0HQL+fT3eMkh5P1w5n1p+Za9uA+CPz\nuvehAjfdHt0g03fVf81LW9ixH0SbspKHgYGoXL4baXx/sjo2YW0b1HefKIwQ4rSUcmfo48KCtqws\ntaDNtcM72i7Yu7CMEjykQdWd63cA7A9QTJ1eV7aOR9j5EzU49neYg5kkdZnp/O0IK8Fgex+2YxQM\n9pLMzMS96dtu2qb3bQtSTe83LECPy/Uc1L2+y6BD3A6MqV1qptr1GPXjePb7MwOyCRSTvo9BHIew\n1xxEm7IwrMGBy/FP+hlleWzC7j9JBkqGMQin4eEatKWxPJLgXhx6xUiraGa/grbVK4qRk3h4BSBv\nMeX7tTrePHS3U2KXsCyWrvzLykxLkKKw7S8JO38aUvbMuJlmIINp+qPcWNRjbQFkQQjMzFa1zzu5\nZ4uxXl4weYa/3EN1voZ903PYNz2HcmA2SVePKu4yP9vyq6hJP2xLn7NKfuHyvKZlcS5LteMuRTO1\na75WNw606P6mH8lEBpGwJItlf0nfhylx1O6t6yO3JTgbbBocCmtzVskc+t3hzssyz6jv2+WcSnrN\ny/LY2Nqg7hNP3Lc98gAA91pSXhQG3YClwvWCZevEZCF6wCba6RbzRaI1iuayt63UrkeWxNpRr2tp\nl7pRycBj1o56zs9p63iEnT/lUqs9lXIJAq1Rw70/tRGrV1wfdxn1Cp0aT1PHzmNmtqp9rpnZKsYe\nexmbDxzF5gNHMf7Fl7se6xJAPvzCWe3zT4xXjMfEH7Q+9tI54xK/+Vq953f+elSmm/5Dz54xvmfF\n1ikxBdXBpB/VdpZGWwCkztew9kRVDjnfKuWSccR6cs8WlLxi6GvECV5s7Soarie64x32GaShH68R\nlFagqDLcqv2MOi61/3YdOm7MdvvK6xcjt8n/vZiv1XF5od61D1Z9D8KO/cR4pec6l3QGRve9NV2/\n0jAzW+37YI6pHVHft8s10HRrdf3+ZDloEtYG/30kirAaiXnnv25kcV+i/mHQlpK8ZiiLqiFl4r1j\nWXFdmrhQb8aujbd21MPhvWOY/cJdmBivWAOYH32wiKspFFIHws+f+Vodv/HsHKrthCibP1LCkdPV\nrgGAhXrT2FlSZmarmHzuTNffXV6oY/L56wGPy83TdsN69J5tPcFBcLQ8TmIZ1S5T+/zBpOkmZeuU\n6IIaf7tdZ9OVtDuHM7NV/OiDRePv1fIkWz02f4c4SjAVxrZyV80S+5lmTyb3bIFX7G6XVxSpps0O\n+5yzECdQDJ7Dj8yc7eqE65ZLh70Pf0feJGrnOex74b9WmI797q3rO+9VJY1689Dd1vM5SfvUIE+c\nTqytA6yOr0k/+wlxAg2Xa6Bpz6Hr9yfLQROXganqfC3yZz5s5UT8+j1oQdni8siU6JZ1DCOXYCeL\n5CODVhQCX/rsjp4Ogu2iXG/KngyOBWE+hrblFGE17oDrz9uQEifeuGR9LKBfcjJ17Lw262S9IfHQ\ns2ewf3rOKSMhYD42WaU+Vjf1sLpyB18815WApTpfw37f0stgGQfVKQlrd5z9jLV6A/um5zqZOV0S\nRJiYPjvFpQPhz24Wt5C2bsnV+5bVA+o9qyXGRSG6Oo89xyL4FhNeanTtVXsY+7VcLuqyP91yLNP1\nIUoiGZeBh6idZ5fzzpZSfffW9ThyuprZ0jPbII96vcnnzvS8nilxjG2ZnO349rtmV5xAI+waaPpb\nCWD/9Fwn4LZ9bv2qh2m7Xkc9x4atnIhfXpbqUjoYtKXE9WIRxa5b1uGt79dyVzJgqQVsgPk9RS03\n8OFV5oyNtgtl1CVJroI3WdsNWx0D18/XdMMydXb8ZRxGvUKkfZ3+m3rYAIlu+bH0/c4riM4y0mCn\nxJayOclgRbC2n3otHdM+lLDOcdQORJzg2rS3w/Q9USPwur2SumOhC0zrTRm7g6Fr7/7pOTx4x6a+\nJraIeqyjzOo2pXTOrBh2DsXpPLvslfTvgw1+x3YdOp5pp9KlffWmxMMvvGYc0FDn6iqvYG2r7fj2\nOwlJ3EDDdg20HUvXsjBZ1zNT7Q9L7hXlHBtU4ew09mLmeZYwq4RjSxmDthSpi4Utk9qVq4vO9bXe\n+n5tyczgDQP/bIwQ8ZbwvV+r48m9Y8ZkHP3e6xC8QadVN9BW/yrY2QkeC1W43DYr6RUEblg10hNY\nqYt4ku9DvSkhZW8GTH/CGRWg+WfG0hqssHUWbBvebZ+dv3Zb1IKwUW6CplHblSOFnkQ5AsCDd2zq\nfG66OoDBYxG1gxF2U9e1VwJ4+uQF57qJaQkO7D307Bnsm57Tzr5GuR5ECdZt51DcWWCXe5Raugz0\nduazvia63kNr9WYnsDSd56bnUG01Hd/gUuQ0OqNhz5FFoOFyLF2CoajXnThcBtLfna85fRZZB5o6\naSU/yessYdj7Y/IXPe5py4BpXfindtyI1Su742RbIgvV4V05Ej2xhmm/Cpn5Z2PiFvNWxaorhgui\nALRrybO4gOpu0JN7trSSzSRQFAL336a/6boGVPWmxIdXeZ39VeVSK6mLSj4w9ZkdmP3CXV37Wlz2\n47iar9W1e1HUcwdnxmZmq8bPNA5Th9S2lMW0X2PUKwDtQYas9yyY2v1+rd6TQOLJvWN4fGJ759ia\ngl7/c0bZ7+KyV8O2nKvfSQRczjElyvUgSifcdG86vHcs9v6x4F7JcsnT3q90+6nSSGyhnse0zyzY\nPhvVvqgBo2qr6Tsa3G8bPG/3T89hc4T9dS7nfhZJXVyPZdKAO63EGRPjFZw4cKfx2r2m5Dnv91LP\nldZeyzBpJT8ZxB5eF2HvL+n7X6rJVzjTlgGXdftA64vz6D3bQpdURs02qUZMJ5874zyrZ3qehWuL\nsQMYpeQVcf9tlZ737xUEvKJYMjXrgkv49k/PabfnPPRs7/6JtGdUVdp/oHtZ4uSeLZj6zI6uIt62\nGS+dhpQ4crqqnamIcrN+v1bH3KN3OY86mwLCuMsW/aPBtmBT3Shc9h26MnVIbTNNptFe3fUjqz0L\ntlFb0+h5WCBfEKIzQ2i6TvpLb6j3vnBtMXRZ3SDKMpi4nGOq3a7Xg3LJi1XSI+0Zg+Bnf/OBo9rH\nBY95Goktoo7IC2FOnKPOFdN5Uy71Ln/3X/fV69lmldV/+6lHmtoe59xXz5H2NcBlBivJIGQaMyzB\n42W6rgjR+1nkZb9XWssaBzFL6ML0PlSimCTX7aU8S8egLSOu6/b3Tc+1MrkVBBoJAiy/+YVrOPX2\nJSQNhdKY0RAAVnkFPH3yAtaUPKzyCphfqGNNuxbXMAdsqnCw7kI4MV4xLpFsSInJ587gsZfOdS3/\nsxV4juqDehOn3r6k3eD/xH3bMffoXZ3HuhaG9zPd2KIsvywIgc0HjnbVvlNJAYLHRv1OJ+6yRf/F\nP+xG8O58LbV9h7ZRzrClLP79GlPHzmsHBpQo55Fr4BxnyVXYsfXPOD198gIk0LM8FejdD+fyeqbB\nE8C9Y5nWvgqXc0wJdrTWBGoXAq3jfvDebZHbYevIp/VeXZdk2WZCXV83LNFCz/XNcrlQKyF057lX\nEJ1Ovm75tDIxXsF+h1qVJsFrq64DatKPgYiw+0XSWZyoiTPCArTqfA1HTldx/20VvPL6xa5zO8nn\nlLU1JU87YL+m5F5mSOnHctSoTNcIAfs57rKXXpdMLS/BeFIM2jLk3yNjI4HUAjagVZstrRmBpCSu\n7w2br9U7HfQffrAY2tkueQUAIrf7+bZt+BCe/u9/2vj7iiWAqTdl57gEi36asvp9fNMafP2NS07J\n9Gr1Bp559R2nC5dLB1FHd2OLMmOo2hZ8leCx2Tc9h2LIkk5T0XPbbLH/4h8WbG4ol6w38kq7szD9\nzXe0x83WyfNzCYpcg+xiO/lDWCc8yqik+rd/pnaVZ19lHyWQV0dOlQlQ7dUNetlez9/eU29f6gSD\nimvHMs0RW5dzzC/Y0cp6U77pvZ56+1JPZzfsdV2De1MCmyhLkcNmJKLsgVXLZlWSmuA1UbU1eH4G\nhQWtYd8H/3uK0n6XgYik55GtPf5BluAKD9fXCJuBCcvmGfyuA6373iuvX+ysTHh3voapY+eN59+g\n93sB5nK5S2Xni+4aYbqPK6576V2W4g8rBm0ZiTN7sRz4O2VhFpsSez/xY7kJQINOfuey9fdRAhh/\nMGVaXvvMq+9Eyn4e5cJl6yAC+gup7sam69QHRV2OCYQPauh+67/Ah3UgbcuJVZ0w0wCMP6V9vSG7\nbjxqmaouUNJ1nFyWsrh24tQ+Gl3pA3/wGHVk+9Tbl7pS/F9eqFsDmbhLf12z8vnpbuqPT2zHzpvW\nxeqoppku23YcXILIOKPlulkIUwBmeq/+TrBr0OpyHpvqDkatyxfW8Y7aUfOXJ1Dt3XXoeM/1zHYe\nhAWtYd8H/7U1ybkflMYghKk9AtAOOtpeQ3cddJmBsWXztK08mHz+TGdQTSXEMpWA0QlLVpWmecO2\nFNPPh43uGmEbzLAdZ9d7Yh6C8aQYtGUkboa7YAa25azekPjqmfdCR18GpSElbj5wFGva2SaDmQ5t\n+xt0TBesK1cXMf3N3lmzMKa9XhvKpdDRVtX+R2bOaoPmYsHcsVJ/O/7Fl7WdqRQnlY10wVLY+33s\npXPa9q5eMXJ9r5Hvpg+0Opi7t67v6qT4354uoDFl2HzspXOdNts6AK6dOFUPzU/X+Y6yd2Jmtmoc\nyTZ1YF0CeRM1um4rveFSpyzu8qA002UH9wL5O327t67vLHdNK/V1WK234EyaLZW7n2vQGnbMdeUd\ngO7vWxiXwM+W0dF0fQyKeh64Dr5U52s997dg0GDbX2danm+SxiBE2Cyi62vozs/J587AK/ZOJen6\nALZsnibBVRD1pnQ+jqbZnKz2SyXN+jgM6fJ124hM5zpgrgXocj3OQ/KVNDBoy0icm7oAsHKkd+Ro\nOYvayes3ie42Bi/g6sLiOtOw7Qv/N37h491JW1yOgW60UJf8peQVe4IM203nmVff0b5esym7ghD/\nMiIVwA4y0B4NdPxcOu2mEcyuwtGazDJHX3svUgps04BO2IyV4lI70GXwR7XL1Dnw19ZSTIkjAP3y\nJf8ez6lj52N9n9WIuIm/TpnKGJZWRyXtdNm68zCr1NcuA4fBmTRXplTp6nVdjr/pHjlfq2PssZc7\n54pptlq9VljgZ5r1+vimNTjxxqWev929dX3Pz+KcB7Zrjv93cVP3H7xXf0xs0hiECJtFdH0N3flZ\nb8qez3NtxFqpQPgyOz+VECtMlERCaUhSsmFYE3Ho3nMBrWuCuh7o3ottYCZsQG/YMGjLSJx6WIWC\n6OrU5HWGaVgkKYacRK3ewMEXz3XdiE0dhKC4+xFXrxjRjhbqloWZRkL9bVbBl+n4SaAzy/jDq4ud\n5Yt5CbLjDJq4jCDrCj+7dCj87bFdF8Ju/qaZBT//cs2wa1B1vobDe8e0gwq62lphxzW4fMn/t0kS\n7NSb0pj1T30+WXRUsqh1FRQ2MxE3MUPUfYRRqFTpwVkSCHQtP7Mdf9s9cj6w/Hby+TOdf7ssp/IP\ntJhmvUypw3UJh8LOg5nZatdMsi3QDAobUHKZtXOVxiBEWHtcBoFmZqvO5+foihGMrhgxzsDosnne\nf1vF+T4q0Zrhsc2wuXyf0t4vleRzTzKjOsgZuuB7NiVjccm2W/KKfS9o3w9CDqBTCwA7d+6Up06d\nGshr90PUPW0rRwq4utibSTHvgVtRCKzyCrhyLf3ZwZJXxCqvkLjkwHJxeO+Y0wXq5gNHc31OpSHO\nCJspAYy68Cc5bpVyqZPc4JaHv2YdTBDoLfyt2FIh+/9+TcnDtcVGaHbWohB444lPGotf+9vvGgjq\n2pPW+aZbSqY+H9vSGpeRdJOwTswjM2c7SX+KQuCB2zfi8Yntzs9vOq/UeRD2+2Bbs95LHfW6HPwu\nAualgbZzRddBN/F/30yiHFfAfB7MzFa1+2G9osDUp3ckXuaaJtM1TpdZ0XUm0OU1gq8VXAFiIwA8\nqRlY8goCN6waweWFunYfcdRrlXqOSuCzdf0+uZxzcUTZl6rY7lW2fkLYPTDN9+Fy7tvud8Hv6aC/\nW0kJIU5LKXeGPY4zbRkJ288UvJGZUs/mvXPdkBILKQZsanZM7cdZOVLoS+Aa5TXyGkjvm57Db73w\nGlZ6RVxeqHcdS//eGV063KXGv99g8rneungmweXJ/syItpH94PLUrt8VBBauLXZqkYUee4GeZYmK\ny2hucMmujWrLxLg5TTlwfdYkaodLtScNug6+v8i7bbmd6XgqcW/4wT2fDSk7/1aBW9hzh81+RJkd\nibqXOuq1THWIbedKUM930TcbJ9HdWU6rXqluiWNQ1Fkn04yYaYlmvSFDU/fvn57DqbcvRQryk9DN\n3uzeuh7T33in8x7810ygNzujy+y1aZuHKauxjaoB6W93MJunbh+xqTasCvSCdPt9Xb9PWe2Xct2X\nCsDpmgLA+vmlmXgp7H24nEe2+12w9EHcfcvDhjNtGXMdzXMZQV8Odt2yDt+68P5A9vWVSx6uXF1M\nVJCc+k+gtc9rXnMD7zym3TM0jSLrRv39z//gHZuw86Z1xppf/s3s/n19rexmzZ7nCzvDdKP0QPrX\nCf/osMtz+2fcXDJ+xaGbUbGVdHBpv20U3DSavnbUw9233qjdF6pGnm2zpmqQxPb3ptcvtN90U7be\neyFQx1Odk8HOvm2EvaIZpde1z0a1Pa16kv62nThwZ+rnt23mM8qsgi3wth1z/33e9N7UTFLcma2k\n/PsHu9olWt9F04xqcEYq7Doah+nziHutUoGVqYZq8O/fna+Fvpco2SOjfrau34fgaoKwGULT9TDs\n+hH3XLS9j89prmMuf5fHmewkXGfaGLRlzHTSBb80M7NVa6HcINvIflb68ZqD2ocGXL95xslyR4Ol\nbu4uN2PA3CE3UeeG6flNS9VM32mXjo0uqxngntQmTPCm57oUKHjzTtLRNi13BNxTQb/lS0IS9vno\nbuq2ACQsWNx84Gj4m7T8veJvly7Q19F1aMPuN2HJQ1xqNKrgL5jAxCuIrlm0KPyfj2mp4Q0r9TMk\nLkwdQ9uSR9sx8S8pDEtTrj5n27ni/3xMtatMneakHdW45zAQb7lj2D0+7P0Cbkv8ky4fFgjPTWBb\nyh7kMkgQ/CyjXFeDyx5drodBYdfyuEslwz4v2/fTNfjMcmlnPzBoy4koJ5LrxdO0Ib8f8ro0MC2j\nXqGzDyhOPTHKnummH7a8KijNc1k3chl2A4za3jgBTZhgx8i/P8ulLVH3fQSfw7aXxs81MLQFWabN\n6nED4KTnz1uGzl7Ynkc/XfBnut+cevtST3IG22h1WOctGFio2Wh1bkZZhh3sfAWTetx964346pn3\nYg+mqb2bLtLaF+g/ti6Dsm8dujtypzmNjmqSoA2I1h+JEuTZgjbXmbao12Td64cNBLrOQLkMqCQ5\n76K8X9NMW9yBuzBhx932/XQNPqO+17zhnraciJIByLUTlzRgU8FInFmtpR7D+BM3MGDLHyFaKd51\n3p2vRUoPneYSHt1+Btt6/DjLwWr1Bh5+4TWsW72y61piqi/nwr+3AACOnK46XRPUPgfg+j4IdT0p\nO8zWADAuQ9Ol7Ff7fsNapvu92lOou/En6ZgnOX+KoreEgQpWolyTg+eY6X4DQJtNr96Q+K0XXut5\nfNi5qav/98rrFzuj5aalcrrZuOD3J7g3JY0gKsoxjVtj1S+YPdJWKkNxCZ5q9QYeevZMp57fwrXF\nxHuQ4qTU93M9tP6Ovgrubdc+276nzR+x95VMBdqjZHh87/0aTr19KfT4JN2fpX6e9LzTPX/U7Lf+\n8zXsswnWFrWZ3LOlp8apX0NK475jlUE3bP+pqb1LbdsRg7Y+cN0gqfuCpWnUK7TqoLS/OA0pl/zM\nGaVPjZbGqe+U1IO3bzIuSVIdTtvNIW1FIYyj2qZZMAF0bpqTe7Zol4OZ1OrNrrT6k8+5FW63P2er\nI/jh0kika496fdX2hpQooBVY1xvSOihUFKKnUGrYZnXXpa9BTSB3GWgbUnalGTctCwyjS5qhu9/s\nOnTc+BwL9SYWfOeUOua7t66PVH5E1W4zFZoHgBtWjeDuW290ml1V0giidEGySdLU7boZ1DTTwfsT\nu5hEeb27b73R+jmrZdpxO7+6mT91joYNWgWD1N1b1zvNuI4URKRrcmsfbaNrWXJTtgY6XPbZuwTK\nYYlvkp4jpmsBEK1sgOtnA7jXFgUQ2tG0PY/pWuRPOGS63xTbpSaGda9bEIO2HAl+wWzneJxlPbXF\nZs+oGAO2wRnWgLlWb+CV1y/iZ25Zh6+/cakv78GfUCCYsU/ZvXV9aNbWtDWlNF78dYMwahmZ+hv1\n/+Puo0wraU5DutWb8xOa1/cHSA0pjfuc/B1PW6Y2f2co6nLSzmulcIxckxJEEXzvYZ9lMZCMJErG\nuigdwlq9gf3Tcxgpugc6QKvTGBZgXV6o48jpatfS2qlj57s65cGAzqXtZUM9J+WB2zd2/dvWicsi\nuU4Wzxn2ekB4/biZ2SqOnK4an8dfxDvqvnvFn+E1yGWg2n+tcB1EqNWb2pkbW6Hyh549E3waAMDJ\n71zGlz67I7RfFnaeml5780dKuOXhryW6toTNntn2QfoTZwVXN7hMIgRXXei+Uy7XN1vgq6udCADP\nvPoOdt60DhPjFeP9XtUbHbZC4yaF8IdQP02MV3DiwJ1489DdqFgKXqrlSEDrhrV21Gtl0QukQfXr\n5z64UY+nVhjbxxFlZHgQqvM1fOvC+3jwjk19aWtTSrzy+kXMzFaNF3D/z5MGbOWSZ/3+KbaitBPj\nFTxx33ZUyiUItDr+T1sBfC4AACAASURBVO4d69lwPTFewad23IgsjmKh/aRpP7frgEO9KbF6xUjn\nGOjOFVXYPWx5y+SeLSh5xfiNjkktZX1y71jqx1F1VMI6fKtXFNFoyq7XX2W4xqolpjcfOIpdh45j\nZrYaqXgy0PpsbbPVweOgOo0ugYl6z2pWrtruCKtOuf/fk8+dQdjlpVIuYe7Ru4zf15JX6PrO6V73\n4RfOYma2FbzozjOvIDr32Eq5ZLy/me6//Tx31WehZm91hcrVe7UF2ZVyqWuGbGK8EiuwOHK62nm9\nIP81Mm264um6a7J6j7ZOv0u/zO07dv01CgL4sbWrcOKNS7HuV0Uhet6Di+D5P1+r4/JCvfNd2D89\nh0dmzmJivIL7b6s4XfPUd8j0nXIdNApbQhqkArJHZs4a+yGqfJSfP9AcNpxpy7GwkY6GlF0jYcr4\nF18e6HKgz7U3pPdzmVocUWa6Sl4BK0eKfckqqTawn3r7UuZLEMslDz/4oB5r/56acfvSZ3fEXrrm\nyn8jMH0f/Muzknq/Vsfco3eF7qfZ/JHrI9phI5cmM7PVzD7npmx1OBtSpjpoE+Wp1LEEWlnEdFy+\nVxPjFW0yjSwJtM67XYeOY+HaovF9+xMYReUS6Fy51rvcULc0ybTE9P7bKl21uJLwigJ7P7GxZ0YM\ncL+mVudrTssew9or0Jphty3l+iDwuYTN6LosKTPN9m/b8KHOfwdn81oJOL7rlBlU8QrA6pX2WcRg\nltndW9db9yT568eZOsMC6EneMDNbjbU6JGzpoDrmaReGN71/03YV0/K6YCigy5waNuutO1+aEvjL\n710x/o3tWCfJihj2vZMAnj55ATtvWodXXr/o9HnbAqOJ8YrzTHMw8FXfIVsbavWG8Z5gW5E2rHvd\nGLTlmMtSr+AFcWa2mvqMWskxBbXy1MkL+INvVbUBW96WBB7eO+a0lG6xKfHEvdv60mlstJMDxO0E\nRpE0CH13vpZov1FU/oQXQS7Ls1ypwp1h38ETb1zCT/72H3Z9Vv5j6rIUwyVJQRKDrjvovxEnWSYW\ntpRLJ0kaeqC74K7NSq/Yl+9rkJql9Acaus7TK69fxNRnduDhF16LdC3XWb1iRJuee9eh487nsQqG\nk5IApr/5jvXzDXYEXUbzbfvQZ2areObVd7S/+/oblzqzC8Hg+T+evICoR77eBK5cWzT+XgBdg7au\ngY96r2H7rPyBZ0GI2Ncp3THXLVFVNQCjZh/VEe3XcA1sHrh9o/beXiiIzvOoa1CwVcGZG//72vyR\nEk68cSly+5/cOxZ7INDGZdZLwn3JvoB5ZYt6LZellv4BsjTK2xQErDUlo54fecGU/0MgrMaFv8aN\nqUgsEH8z/lJPfa8rXmsSd1/NsAtLKtHv2nrBETQ18hhnz4VOMBW6S10gG1Oh14nxSuKU23nnr8ET\nZzRdpYOOmva/4psB6tcexzDBfWlpUTWaTOepqjOYxkyGqcZT0u9IFqKcC7bvqOJy/haFwIdL8WvK\nReVPfOK6ysaWZt5fWiStma+iEGhKaa03qStnECc5T1CU+nb/9A/Odma1g+1X+9qsWSsTDhL5X8+1\nREVUaRSwV/f8sEH4YCkP//FW+1d1WWZLXhEFAe1nEYVAawDWFHzmqRwAU/4vIWGj02pUzDTLMLpi\nJHKWOr+lHLABrYtFoymxekURC9ca1ovQcgzYgOtJJXTnT5qdYX8H33RzUZm+/FSWsdEVxcQXeqB7\nCRGQPJFAdb7WtVxY/fu5U/1b6jcoT528gFdev9jpOJ16+1KnFlxRCKzyCtbPTCWRcN0X8aCmUOv+\nPs0EhykA+PCoh/n2HpK0qFF+0+xEmrPQpr07/U62EcbfEVQpyk1KXhG7t67vmR3bPz2HU29f6pxP\n//QPwoOYOEl94vLvr5qZrTq9brFwPR2+bSno+BdfTm2pYjDxUKuAfHipgjTuLbrVDqZlxLatKC4B\nbFqrGoKJc5IKLt/3iiJ2YKmyR7vU8/TPnJlmr3X3+bTOO7VnzyTNrK79wmwRQ8C2iVmt6Z+ZrRpv\nmGoJ2w2rGKPbXAkJ2JazcslLP5uFxtMnL3SSJ+jO+wJaF2HdEq+GlLhyrYFiIZ2G+i/ok3u2JH77\nwZtkvSFjLZuJyiuI1I5JXCqpxE/+9h/iqZMXOjf7hpSoGQK2ggB+/GOr8cyr72BzhFkcXeKDqIk4\n0qA74vWmxOiKkdBEU1FV52t4yFD+Qe23SaODotu788jMWdzy8NdyFbAVIyzlU2U7Xnn9orb+3NMn\nL2BmtopHZvSzMIMS/CxcEys0A4GFP8nGiQN3dpYBZhV41uoN43P7z9GpY+dTGzCu1RvYNz3XubeY\nlhHbkmrV6o3QpDhpUUm3wqikQ5sPHMUtD38Nm33Jh/yPCSYegUQnsU4pYtK4+2+rYPqb4QGbEkxK\nEmx71OuGShIWlnjPxSDuC0kxaBsCtgxLEsD0N97B5PP6dLXA9S9l3moV0fB4v1bvS1IZf8IRAF2Z\nvsolz2lPSLMpnTNa2h4XTJs9jAH92lEPe39qYy4u9PWm1O77Cv5EoDXj+ku3b8Jffu9K5NF2XWaw\nyT1bWkuX+mTtqBeaGtxfYygNpmWXq7wC9k/PoeD4nTA9SojegugqwYLrZ7R21Oss189KyStGOmea\nIXXPJIB903N9TYATZvWK1mzH1LHznSyhrp1fCXRlkPRTHel+7VEO8neiowwyuH63/VkOdVRyN5N+\nrbBWKzFMgc7NB45i7LGXMfn8mc57Cc5m2jKE+gePvv3Pfh6f82WBLgqB1SvMx+DpVy9E7gsEr8n+\nQDKqT+24EZN7tmCVV0i0Jz9KyZQ8ycO9nEL4p7Z1nUx/wWyd2mKzk/kpC3lKTp91hyAvskiRbDKI\n5DH+pTKTe7ZY16UHSVzviIX50md34PDesZ4btS1t9jD5oN7E0dfeG3gykigkWiPNpmQPLoIdvn6v\nNPig3jRei1TH1FS6Im0qnbdrIGN8lOxeXrbr0PHIgczoihE8es+20PT3XiHeNW7UKyBOuoxBBClF\nIfC5OzZprz9havUGpr/5TleK9Sj3YbX828+1I53GPbZc8ozXXMU2C+KfbamUS5j6zA7ndtlm1FpZ\nouN3i72CgBexvqFJvZ2QTNHNmJn6ff4gKSzxjkqu4l/9cG2xaXwfcQNXld1ZDQrEXQL51TPvYfK5\nM5ETKgXPmbjZNweN6+VyLrj2Os4abymzzVAnEa/YdyZtkflJnKIu4EmyyumSpHjF1p6EfnQ0hOhv\nfT8/dZGPsxfTKU1xe3TWtq9j16HjQxXwBNXqjVx8L6NKWsh6Q7nUs/G9nysNavUGVo4UtAlzVMd0\nUPspwhIH2bKzAvGSySjV9lJ9wJ6drt5sPbZSLmF0RcGaGt0vzrW2n99utR9IlUt45fWL2HnTOjxx\n3/ZI1/OmBJqBDrtEtAE2f6c9LMGGUi61inObEj6p8gO251JligBzWYWZ2SquXNVnzfSKoqfMkeJ6\nXpqzcTdjZVcVQFeSFVPCDZ21o57x2rTgKxIedU+qa4ZQ00ycCGlbVGtKXiqJbVwGUHXXXdM5M2wY\ntOVcWpvHo05DR7n4+7NuDXpfwyBmREzZOetNmbjDL9GaDvefAY2GxGMvnUv0vM6vP8B4RS1Lykqj\nKfHQs2ewb3qu01ENZhobxo3KS8GGcgl//f4HsRMRBBNPDOK69H6t3pW2O9gxHVTijqaUeMuQbVgF\nFUdOV43BZpJ7kkqzDQBXF8M7x4O+n6TBn8Fy99b1XfXy1F7Pqc/sSCUzsXqdd+dr7WQf5mOsBjZc\nO9L+jq+uhqj/97ZMosGC3UG2Nq0dbQWNwYye/iQbq7wCLi/UrYMTUVePVMolLFxb1AYwugyEuvc1\n9tjL2v5J2CUurKaeiSpbo0u37zJ4JNFb4zCukleEEOEJRtTsepLvgbqH+weFVkXct5dnDNpyzuWL\n6pJmNupFyvWx6suvLlKDWgs/SFmO4Av0ZqRqZvyafnFmLfNWi8/GvyQEuL4f4NTbl0ILi5YjLNkk\nvVGv0LO8W11T+l1IO20byiVrvS+X2kVZtQuwzzDvvGldz8+B5OnCJa4nzBjGGWAXKjmCujaUfYHG\n2GMv91zP602Jh194DU/cd2vi80EFEKbi30pBoDPQ6vJ6wcGsnTetw1fPvNd5jwXRvSTPNCBRFAL7\np+cwdey8sdaYqU264CgY4M3X6ih5RRzeOwYAxlUaUe5PqtC4btWHVxCd4u7B71Bwpt90r5iv1THq\nFYyzxGEzZiZXri121SGLM3hkq4saVBDt9PoLdawpebi22Oi8JxVI23gF0QmMTX2Ikle0PpcAOtcq\n/6DQ5YV6aL3UYcE6bTlnuknq6p70e6arIIDf+exY50uQRv0PWhrUaG/SAql5NkzBaZpGvQLWrl6Z\n6LteFAIP3L4Rj09s7yR6UZ0a/2j6g//mT/uSYTNtwWVwps5clL2aYUvQf/xjq7FwrRm6NC3OXo5H\nZs72zKwsZV6hdd2yDVi1So80nWcqw5ZAqkAjycBnp17fw0dDZ3Bclr6p+n5RatWZjoGJ/1oAmOv9\n+esDhi3prJRLmF+4lkq2T39NO3/ZFqDVByoWRM+gU5T3D8BYTif4+rrjvqIocM0wYO9Sh8xltjXs\n2iME8KSvL6h7Ttv9UrcNRD3e/3drRz3cfeuNXbPVfqpkkKkvmqe6bEGuddoYtOWcrfil6cabdpHT\nkmWZhX8Ezva6h9vLhLIK6vKyj41a/MVsl+Ps61JXLnm4cm0xUUZRgVZNtZ03rbNe44at+LjqWAQ7\nbbpOiCvVebZdQ9VjAP3enlGvgJVeEfMLdW3haJOZ2WpqReuHgRo0eOylc6FBzefu2NQVmO/eut5Y\nvyps+aPqUMY930X7xEp72e1bgULqLoOzwULlLoN3Lh3u4LK3rPmvQ1EGpV1np7pfq4AP6s2eJaf+\nvp5u8CTseuLfb2f6vs/MVo1F58slDwfv3eZ07YnyebswDYx8fNMafP2NS533vXpFEf/8F64fJ9t3\nyOV4DIJr0Ba60FMI8e+FEN8TQvyZ5TE/K4SYE0KcE0L8f1EbS2b+dP8uWW9mZqvOqZ1d2dbF+9PL\nmrI9lUteJwtg1CxZrhiw5Yt/maHL6eiaon+YVMqlTLN8fu6OTZk9dxhV6yfJpybRKrz9G8/2ZhLz\n11UKk7dz5/JCHc+8+o625pf//0dRaCfNOXHgTuMxV0sPdfeMz92xCRKik0nSVDtJJ24SqzQz6WWl\n5BW6jtPhvWOY/cJdmBivOC1BP3K6VU/yzUN3Y3LPlq4sfEFhHX71+6h1sxQpr5dMSUulve9NpZl3\nDVxUbVhV/82lA6+yxer6Cf7i52kEbLrzcu2oh123rOtKfX//bdeXN0fZUxYnYPmg3sSTe8e6ao8F\n92LpluyHvZI6J/ZNz2H8iy9rv/MT4xV86bM7tKUTrlxrJYSxlShZU/Iw+dyZTmZL2/uPckVo1WTt\nvTf4A7ZWGxv4jWfn8MjMWTwyc9b6nFGvf3kTOtMmhPgvAfwIwO9LKf9zze/LAL4O4OeklBeEEB+T\nUn4v7IU505a+JFm9klKjYKYpcTXyVC55EAKYX6gPzdI523pzSs4rCkx9egcA9+xfw8A045KGAoAP\nc08dPteeqVtOM0FhCRbeDMyMAObZkXLJw9yjd1lfL87KDTVbBfR/2X4UpuM1M1t1XiGgZsjCAhqX\n2ZfDe8ecZvj8s6ZZ3ke9QiuTZ1T+ZWhRjqWaBVEZF/1Li13PI5eZ7OCSO3Wu2mb8TYlEohCA8fMy\n9aH8bUhjFZVtpdb4F1/Wnnu21VbDvE0gT8slU5tpk1L+EQDbpoJfAvCClPJC+/GhARtFExzpMo0O\nxMnqpUaVko6HqpE1fxFw/5dZXaTma/XOiNKXPrsj4atG9+MfWx35bxiwZUvVDJoYr+D+2yq5mzmJ\n6/JCHUdOV3H/bZXOaH7cUfSgJgaTKTVvpr/ZGp1Pu9PQxzrckVk76ALa+4Opsztfq4eONtvqZZmM\nrhjpJGFJs1OU9sdiem9RsvNW26VJbDMxAm6zLw+/cNZphu/qouwsc81y4DPOrc+fDEINJLtSsyD+\nGUx1/kQpHh7lMSpDoq7/5E+uUm8k7wdIAA/cvrHnPFbJl8LaEOe7GOR/vkdmzuKWh7+GzQeO4paH\nv2Y892yrrbI6+0peMfO6u8OYHTqNHsTfB7BWCPH/CiFOCyH+G9MDhRCfF0KcEkKcunixP4VFh12w\noGJ1vob903PaKWDbCaib9vaKAl/6bCvVsO6LVxTC+Uvjz0h24sCdWDvqGb/M/sLJaS/xsnX4S14B\n3738QaqvF1Qpl5ZNge80qZpstuVFQV5BYDTnqXxr9UarGGi7QHicGkBkVm9I/Maz6e+ZHNbl1lK2\n9pBuPnC0aymU7boYFqDolqsJ2AfAgh1s/5KvJNL8WPzBhd/MbDVydt6HXziL0RXmpf8qFX8YW+Fn\nv4aUsYpq94NapnvzgaN46NkzsVYZqOXRNx84is0HjkbaFx31eKj+iKn/VJ2vYeyxl1NJalIueThy\nutqzJ00tw7QVwrbVrovq3flaJ8NoMINyHhSFwBP3bcej92zT9l3TkkYQ3G9p9HhGANwG4G4AewD8\nthDi7+seKKX8ipRyp5Ry5/r15vWxdJ1u5EUCePrkhZ4RUtMJWCmXMPWZHV03zrWjHqY+vcN6oWhK\niUfv2ea0D21yz5bOjODmA0dDb3rqpv74xHZ87o5Nqd14GlLi8N4x7Zr4VRkXAD+8dwyTe7YMtLaZ\n2r+S5V6qLGwolyLPFNebElcX83OjMZmv1TH5/BnOjGVkWAOsrF1eaJ13M7P2gRDdtdq/umPq2Pmu\n2eJKuYQn947h9r/7EeNz+uuxAcDBe7PtfMXhDy52HTqOR2bOYteh47ESJ9XqjdBOveuofkPKSHu/\n83b6C8Bpb5OLOH8d52+q7eQZJmldu3W1yiRae9UAexCxb3outXao/cSu+v3NbUh5fflmRi/ur1U3\nTNKo0/ZdAH8rpbwC4IoQ4o8A7ADwFyk897JnK3yoZqsU03ro3VvXY+rYebxfq/fUW1GJS3QXV3UB\nWTlScOpMR9mPpG7qE+OV0HpYURSF0NYl2b11faY1n3bdsg7AYPdkrR31ujJ2xakjtuuWdT2bfLOm\nbvJx5Gl00CZJlsUowurYJHm8Sxp0ypd6Q+Lgi+dCMxfefOBoV/kY/3VMLVfz74OZma1ar6cSrU6m\nCoBUdt80Cx4n5b/uVOdrmdcEjPLeVo4UOt/LYdsz5NLWONkVwyQ9TlnfSwrCXF9V9fN2b12vLa0x\n6M8/i3Mw7Fqgyjqkee9UpYjymD3SlVPKfyHEZgBfNSQi+QkAv4fWLNsKAN8A8ItSSmO2SYCJSFzZ\nNjfrNlEHCzru3rreWDMGMAcZUWuNuNR8CVKbQF0219o2wgYFUxT3o8aQmtka5Ib71n4pkSho9IoC\nez+xEa+8fjG3yQNITyUncP0eqvpIO29aFzq7oGoy2hJ+ZNERGzZ5LT2y65Z1TvXuBIDRFUXtrFFY\nvag02OpVuRq2IMfEf5+Om8wlyn2TBseUhCRPCmjtpU5DySvix9auwl9+70pKzxjOXxIjjwFbanXa\nhBDPAPhZAB8F8DcAHgXgAYCU8svtx0wC+BW0PtN/K6U8HPbCDNrc2GrkuGS+sdU8AexBRj86YYf3\njhnrg/ipmk6mGjh+/tnEftUYUjP4S6GzoAq3D0t2T4ofsHsFgRtWjTgFeq7f1eUsr5lm07iWq1pM\nWZ0DahAhyYBRXo9/XOpavCZBXcSVIwVcXcz+mNgCfjJzqcEYpgCgWBR9W9GR1OoBnydhtY4HgcW1\nlxDdTJHrSWeaxRq2ICMYoIaN9qrj069003mYaVuq8tYZsI04DtvNk8iVrtBt2gSAn7llHb7x5uVY\nM25CoC97itX+cO5T7ZbGTOly9Nahu2On8/evmOhn4fFBU0sdEfM7n6d0/0CKKf9p8B6f2I4n9445\nF9j2M21s3VAuDU3mHN2GUVVewJSVLCwjVBbty7J4+HITTHqQl4ANaAVsptT9TQCrV4wsmbIFeTIs\nRzSPGU2Tno8lr6hNopA2CeDEG5did/z7EbCVvCIO3rsNq1emkRJgaVkOAZsaoFXfqXLJS1REXj1f\n3P5YQ0ocOd1K+pPVOdmP9PtRqIDryb1jsWcehjHdP8CZtiVPNyPlsqdt0NS+hGDilKCwgqZhG/CT\n0iV2yXMx2WGg9k+p45jHvVJh+2bCksBkfV4uNVH3KcXd15TGTEEeZxtGvQLqTbnkZ4CjXCtKXgHX\nFmWka4v/ep9GoWMaLroVP0nu9/4VU0n3iqqZpyxqVv70312Hc+/+MFezeOUES4YBzrRRTvkLXgdn\n6YK/y9PsgPoaVudreOylc8YCsGEXy80fyXY2cffW9V0BpapT99ahu1MrpLzcNNu1ptRnG9apWpFg\nlDOusNuE7eZWEPHOyzRLYwybqLfluMuMbliVfKQ6bwEbgNY+L4lcjZZnYZXjNbcggMVmtIBNrerY\nPz2HscdeRo5ul9Qn/hU//hq6cahaZKr/MDFewf23VWJf46sZBGxA63584o1LuQrYgNY9Nm7ANqzp\n/gHOtJGPy0jPoGY9vKLo1JVTZmarsWrqpEmgtazBn0YWiJ/ti6LxCsASyjtgdXjvGAAwGUhGhm2f\nbxzce9uyFDJMegVgsTn872OYHN471umDhK3ycRFMQc9+Q3b8ZUceuH0jHp/YPugmdWEiEoolLBCK\ncrNbO+phfqGONSUPwlKjxFVwOjvJRTOr4NMrCED0ry4XLR9rRz3MfuEuLsvKSFEIrPIK1v2TS6Gz\nT0tDGlkHB2VYv0dFIfClz+4AgIEPGA8rgVbCoEEuSPAKAlOf2TGU2SO5fou6TIxXjMski0I4bZYV\naC3lmv3CXXjz0N04eO82jK5IvuwouHE0yUbShpQ9SUNKXhGFhEte0tg3wlU36Rv1Cigm/XAH7PJC\nHTOz1aFJIDRsGlKGJrwZxo4mLVGitVTzytVF2FaFrh31sHpFvhJkDev3qCElJp87g8nnzwy6KbEV\nRHrJkuJsAZFo7VkfZNK2elPi4IvnBvb6STBoox4P3L7R+PPJPVusJ43K9qemnpOu+/bbUC5hZraK\nXYeO4+YDRxNFN2pvnz/75CqvkIvCuDlowpKzUG+ikYcPN6HJ585g99b1zFJKy8aQj7VkRsrWvWK+\nVrcuEb+8UE+cfVf3EQzLxzJiOIHixi3DntBn5UgBMoVPzysKPHHfrTjczmzuSqC1fWTQCfDytkfP\nFXPWUofKhPTufA0lr1WQ09/P/eqZ93D0tfeMNaqKQvRk40nry+kVBXZvXd+95y7mddMris7eM3/R\n0csL9aFdtkHLQ70p8fSrF/qS2pwoD3iuD0655OGHHyxqtxIMy8eyaBisG6a90CWvgFpKDU7reeoN\niX3Tc5H3lUtwT20SnGkjAN0zYhKtL3ZRiK76I/O1unVfmu7CnkYtjNUripj69A688vrFxAHg2lGv\nk9BEF1AOy40IyD7bp+nZh2WEdalayp1YnlvkV4lYT1SdP3nIhBxl9iGPvILAlWv6gI36K61AKws5\nbprVsGbSZdBGAPQzYlGXAfhvlGoZo8tfl0uedrnX2lEPh/eO4dwXfw4T45XEAWClXMLsF+7qbD4d\n9tGeB27fmOkyuZ+5ZV1PJ7rkFfHgHZuGvkMSZtQrOC3LykPncCnJY/dw2PdCDrPqfA2Xr1x1Ll5c\nbt8zmgw0YlFHuVIu4YZVI0O9DFAx9S+o11K/ryteUeDRe7YNuhmxMGgjAOnMiKm9cFH2sZW8Ig7e\nu62nltzhvWNdARaAxAkYgu9xmDvcBQHsvGld13FLuy7cW9+v4cn2enV/jb/HJ7bjxIE7l+wFfu2o\nh39x361ON/qGTGN3AOVVueRhUBmWqWWh3nQOHi4v1FPP6uff9xzFsA0Kqv3oqrSIa7bnuMenHwSA\nT+24sWf/OnUreUUc3juWm/t6lrNg/z977x4fxXXf/X++u1qJlQCtBNgGgcC3gE0wkk0SbJLUt5o0\nvoT4msRO4zat27R9GjsuDU7d2E7cmD7EjXtLWj958nOexHXAdkLtOAlOY6du7DgxBGFMAN9BCDA3\nSYC0oNXu+f0xM6vZ2bnuzu7Ois/79RJiL5o5c+bMzPmc762pIVZUPqqeYEwbAaAJonLS55vrXrjF\nsaX09P8Dw5l8bRJzcUk3Lpo3Dd99cWdJbQTGEpkYcXv1PBUzClB3mGrD3fH9zaHuo28gjdtW92CG\n/jC31sirt0mJX/qHM7htdY/v8VHP44i4k85kI5GciAQjzFNWiYQFUYudNsrp+KnVaqXnrsuwdmMf\n7n5iS+SSOygAj2/ow6LZ7WhpagjcvuZETCtMX+e0NSdcRbi5yPdF86bh4Rd31nR8Hj026tnmUjk+\nmsM9T2qZI+tRuFG0EQDA8qVzi27WXjXHjAfPKa0TsGh2e/59J6udQLvBmzHcKM0FJp0upGe37Q90\nTGaSiXhxIpMKIgBiVShE3jeQxh3f34wJiVhFjssIGr51dQ8eXb8TD//x+Vi7sa8uUh4LNHepUm78\ntXxgpZIJDB7LBI5dM+oizkglcdG8aXh8Q1/BmDCu1zCD2sc75kRFtaAjlcScKUm88Mahio/JqAmJ\n8UzU+tl4ZpeSOGzOiqfQkUri7qs0d7Ooibd0JptfqA3K8dGonanSUMr5+jY8jjTB/nIkng2ZnMKx\nTBbJRLwi85r+4Ux+kbvehBvdIwkAbeBaXRRXXbcQq65dmH8vlUzkzdbmG4AhHNZu7AOgTZTtsLo3\nWpOfWLdjJchNN5VMFLn1hZHIxA9xEby18nLcf/3CqvjSpzPZiqxIWXn+jUO4c+1m3PPklsjHOiQT\ncXxNd7GNgrtHEAbSwQVbS2Mc/cMZxETQN5DGs9v245rzOgquAcP1KQoPZeKNQFtM+83OwapM8g1B\nHzZxEdxkioM1E9WKngAAIABJREFUJomV2BcpDePZXGqYhPHsBrSF2Qdu6IpU+EHfQBrNJdSqGy9J\nWAbTGdy4uNPWld+oPfe5NT2RejakMzlcc55z3eDyt6+J+XqDljaSZ1l3h+2qg/W9JSufKXKNM18A\nR4+NFm3DnGbfwG5Vz9iOXTucXDitK0hGnJx1G7d5xDrEdctYKplwXSn0WpE2bvTG/sOOsQhKY1w0\n15DhTNmTv3LcU/0Qxspah8ViG0a8ZtQx6jAZY69vII3HN/Tl3V6i6r5E3Pncmp6quWca183tj24K\ntaZhTqm867yZ7i89HalJ4omKsTiwdmNfWebWdCaLO77/cv6+GzXBU0qtOpHxka13RiqJe5ctwKLZ\n7bh9zaaic5Mp83pvTsTQlIiHvnj8yK978fH3zcLqX/eW3UY76nFuwKUuEhingb57II1V67bbXlwt\njQ1FIsptO3YsXzq3yHJlzmZotqo5iT47DKtcTqm8m8dNDqtSgPZM81r7WbLyGazd2Idl3R01t/RM\nmzQBG794GW5c3FnTdnhhV/A8CIm45IOpAeSLsMcitOJbTYwFkLUb+7D80U0UbHWGAqom2GKi3V9X\nrdseehF6u/vu2o19VfEOIN4oaAuLt67uKVugpDM53Ph/fhl6fHWtCEOwRSH7bN9AGt1fehpAOGLa\nekjDmRz6hzNoKcGa6UY2p/DUy3swcUJw+1IiLlhyervrd5y8wqIMLW0kME4WrxmppKPgGrSZMLpt\nxw5DiBn+6XYxcEaiESOBhvH52o19GDpuYwHUa9EYE1rDzeOa8zrQ6mJx87rtmd1F7OIFyyHoYuju\ngTTWbuzDwxW2kpWD2RJ7xMZS6wdjYcAaTB+1Fd9q4raQQoiB6Asb5a48J+JS4Dot0O6Fp9/xI2T1\nRbHlS+fmEwGQ8cfzbxyqdRMiw02LO7FodnugxFaVon84g+WPbUJMyl8Mcvr7UqyZXpS6uNPS2IDf\n7jni+p16nBpIrdIZL1q0SK1fv74m+yblYZdhKpmI476rF2DVuu22QszITuV3O36DQ83ZICfYJFhI\nJuK45ryOoqQMwFhaWbubQlhB+easXGG6pwVpn2HpC5LtsTEuGKli3FpbcwJnT59UVsIFAfDWystt\n3XdPVDr0hZQ6fDZB9EHemkxgaGQ08nGUTsQAIISJUqWJl5k4yRBkxjPA6R6ViMm4X0RgUpfaEaW+\nf3vl5QC0ZC0kehhzhiggIhuUUou8vkf3SBIYu6QlhtBycmG0xrN5bccP1kQmdvER6UwWj/yq19bK\n1dyoxXnZEdZNv28gjVNXPIW7n9iCoZHSLEh2KPgrGmr0fdAV9ObG6hrh+4czeL7MDHmlBtMnYoLx\nmBPBOPfl1jesBMb17kZDTPC1G7pw91Xz0VLl8RgUNweoTyzuxJv3XY4HbuiKdJHfcgSbYVFbtW47\nli+di45U0vFaHu+CDYiOaDgRqUbf+3EDrHVYxImOnwQmUXw2ekFLGwkds/XLK41/OZRrURGUV5+u\n1giAr93Q5ZroxHCFKHcVPeoYfbGsu8PXuIiLIKcUWpMJDKbLT9ASNTosrsHLH90U2mQ5HhPHuKdU\nMoGWpgZP616HXpbAqx5QMhHDsUyuIufHK+FQWJi9B+otIUxHCffHE8GS5kWUrD1R40QqOWI8d8dL\nQpPxRFCvrkrj19JG0UbqllNXPOXrwegkWIyJrdVFs14euIbr5fwv/qQivuT1REtjHMMjWccaZXZ0\npJI4cPR4zWtxhY2dK/KdazeHVjA1ERMk4lJUdDYRE6y6bmH+IejmEnTT4k5f56iSVPM6N8Ssm9ug\nHbW8FxkLIbetKT9BxYlEpWpLjQdSyQSOHBsd1wuI4516mR95xe49oC/yRgW6R5LIYhTUPnXFU/ks\ni6Xg17Td1FBsJjdcx+xcNG9c3OnoxlSJC6bUOiQXzZuGO9duDkWw+XFXA0pva1tzoqBWU7k0NcTy\ntQMTccHQSDZf6+/xDX2+6rv0DaTHnWADtHFhJkzBBmjubQpa0okCLC+d+l+AqtVMdGNGKllyptKg\nDKQzeYtVkPNwgUf2s0piZBWsxPy6aOzodKSSWube2ifcK4m25gTuu7q4vIGVuEg+proaxES77rzu\niZXMdBgTYGgkeoKtmkPN6P46Hd4A6kOwAd4xxFESbEGgpY1UFT/JR/y6V9pty4qdK1dbcwJ3XVlc\nx826bbt6JmFiHHepmaU6UknsHTwWShutSQScMFbf/WbDtNZMA8pza42L4OPvm4V7ly1wPUfGfsN0\nC6wnjDEOoCaZy+IiOG1aM17bN1T02U2LO0MVkaVgXHsAIj1G4iKYnGwYd+nx7caA+ZzUurZlUKzP\nlO4vPe14zgyLNIBQswq7YU7qUyvX3LbmROTGsfGcqOZ4CyN7IymPVDKBnrsuq3UzCqCljUQSt4La\nQHFyESN1vp01zmolSyUTaGtO5K1Gbc0J29ibZpuacXbbzpUhhmJSvJqciElB+wyhWmow7O6BdGii\n0ujni+ZNc02W0JpM5PvdazXcbM00Y5esxguB5s5wSusEPPziTnTd8zSWP+YsqvsG0ljW3YFV1y2s\nijUlaiun/cMZ3La6B3/zg801EUdZpfDaviGceVJL0er+DzftqbolxajFaFx715zXkS8N0tLUUFR3\nKCpklXJMllSvdKSSeOrlPUXj0ijOHJUaXwJgyentnveqREyKFgHvunK+ozXRuFmYn1+VRinNQlIr\nwdaRSkZuHBtlZqpdfoKCrfbUqyUfoKWNVBmnODSvdO12cTrl7suLUi1CLY1x/N1HtRXjsCyGdoRp\naTNvc/nSuY7JEhJxwaprtVViPxYKp/NmWFP9xvikkgkcH8357iNzYhKDsNMuG8lMgsTR+cHoj/GQ\nPCYugvuvX1g1i4IdRn8aYxuovIXDvM/hkdGSLQxxEZzSOqEqyZIEQHNjvKLxsUYJlu9GuGYkUGg5\nW7uxz5clxrhezePMzRPAuC/WW3IaA8NilEomIAIMDGdsy3N4lQOqFclEDO0tTZFq04mIW1kmNwRa\ncexS7q1RSvVv4NfSFu08ymTc4VVQ2yldeylFX4MW77ZSalHsVHNjXix4WfQM8ZLOZPMPffNDMOYw\ncRe9fet3HHKdAJm36afW1W7dQrWsu8PWxSeTVbh9zSZMTjb4cilzOm/GPoBCd1inh74IAp0HBeDu\nJ7YUiOYWHxNSAXCjXhDVyz02p1TBjX/R7PaC4xgZzRYl67CSiAnee2obXnyzH1mlCtw/DWpZd65c\n0ZFVCvc8uSV0gRQk86txBg1rslbPsbIC0hBsRo3GUl1UF5/Whi273QvEloMx8U4mYjg+mgtdsJnv\nZcbixiO/6g11H2HQ1BDDyGjOdnFtWXeHL8Fh3Cv6BtJY/ugmTJzQ4Hj/MO6LfhfszPdxN3Hn9z4f\nBm/eN3bvs97Dzefc3J+1XLyxks7kxqVgSybiECjPZ08tMY9jgcJTL+8JPC6MsVXKmKrHVP8GFG2k\nqthdZOY6bk4TsdZkAktWPhOojIDXvrwwtu9WvNsO8wPZzdJmfWBnlUIyEcfdV80vEDR22S1vXNxZ\nIHzM8SGGpc/aP+b2OIlB883MyZ0lq5TvSbz15ujUJ9Z+sX7nthJiDgbSmfyDoW8gjRiKYxxjosd5\n2EwwAPdJhoImqMzHkE+x/9gmXxOnTE7h+TcO5V9nlcLjG/qwaHZ7vh3VjrkwMF8rpU62BPbF68ul\n1Ad2OpOt2qSxbyCdHx8XnN5ecJ69EGhJSH6zc7Ci7Z3eqgmpsC1fdjEjxr0sitbjUY9rNehYy+Tc\n75HGfdEuXMCKYEwQelnjeu66zNd9vlyWmBLkWJ9RA+kMkol4kaeD8f9Kx4qfyMT0xc1UMhGKaGuM\nC0ZCXgAwvGYMhjO5wG1NJuK4aN60ogVvPxhusfUK3SNJ1XETM3YiJaGnvrJzufBryQqjZpxbcLkZ\np1IC1jb7dQUt5xic/tZPQphyLTx2CWasYsZwt/Q6njCtTW3NziLNih/XJcO949lt+zXBHkJNHusY\n8Dv2vPBygTMsL9YEMn5dxKz4sW6Wwk2LO3HvsgW4c+3m0ASHucaco4W7hHNrd//y25Z6tARYr/s7\n127GI7/q9T2pqnVKcadnS1hujObte5WtCdoX1oQopbreu7Hk9HY8/Mfnj70OGNLgt1QPGX8kYtp9\ntZy4vnIXtKylaaIC3SNJZLFaVqyfAYWxYHbuWUbyEj8JRcK6OP1Mmg3rhFvCFaM9fl1BSz0G6wPb\ncA8ztgm4x9wFXV1OxAQTJzQ4CqJ7ntxSNHHNZFU+EDzMtrhxLJMrWgV2wmw9c3KRSmeyhZnwQpiR\nWMfAXVfOt13MMPe3n5pzCppbp3UyaFhvzW6ZBsaxB0VL8V0ZK9Gz2/Zj7cY+PL6htHIhdpgt3Kc6\nxD8qFTwLnpMbsVuR4YF0BoN1FuNkcHw0i/U7DmFZd0dgUT0W37KrZgWYrfdp68JXJlueG+mExFj+\nNzcX31IKm/cPZ3Dr6h7curondPFrbO/tg+l8YjA3t1Gn55tft+Z6ZDzEIVcK0f/JlXlZKyAfTlAK\nmZzyNXeMKhRtJFTCsGxZRYrTBMrpoRCmdS0IcZH8CqqTO5+5zeXG3HnhJRy9xKCXO4vZMmH0s7Hf\n3QPp/ETf2I7TRLd/OOMpLq0is5zHop3g9xozRl85rRKH/Zi2jgE7kX3RvGl4dtv+vBvrdYtmYvVL\nvZ4WHetk2EuwlSqWc8p5EmOkIC/VfcsYX5VyHXSbWCoVTgHlYx6ipNQg+1qTU8B3X9wZ2AJqtu4+\nu21/TSf2TjFnYbTJyOy6fschV48MoDxXQq+/SsQEjQ0x3wLUHBu6/NFN3tZj0ayshgeCcV8NcwEu\nStRLkp1aEeb9rFxhXEqOhKhA0UZCw49lpxSCiJtKtQGAZxB4Tqn8Pvy0udyYOy/CSOriFEBujb0D\nyut7L6uksQ3jdbmZII14I7tYRbd2V2OV2GkMWJO3WPv68Q19uOE9s/Dwr3YGcuFT0NLw24m2coWR\nEadpHTuGO2mpfTkjlQz9wWseb25xhIPpDL52QxduW1Ne4ekZqSQGhkdsJ82NccHRY6Olb7yOiMcE\n91vclWo9qQoSc1YKClocMqAlQTH2Ya6vWOn4v0xOIVOixdBPEiqli3eDvoE0bl3dk3fB9rIEGveJ\n1b/ujWwdRYPmRAxNiRgefnFnSS7U452UHjceFuX2cT0nImGdNhIaXjXYSsWurpfTxLZSbQA096mE\nS0EnqyDzarO1zpy5dpsTazf2YcnKZ3DqiqewZOUztvXr7Nrj530n/LbTq++D1kszT9ysx93WXH7t\nNaMWoJ0bltOYsTuv5ZZ8carf54ZTX/9w056SHmYD6Ux+LJn7OgyB2tQQKzg+I71zqds2riO3cWxX\nJ9ELawIhJ2akktr5KXNiNjwyCqeY8kxORX6iGhbZnMLdTxTWyqrlpMp8n67kAo2CJmrMC4H9wxnc\n8+SWimRcjQrGsPYa3fddvQD3LltQtVqb5TCcyaF/OAOF0sVEPKrFIh3w29pEXHDFwumhFjMVBL+/\nG4S5MF4LaGkjoRFmun4zfuKvKt0GczvsgtHtBJmfNgeJV7Ozrtymxy9YE0cA4Vry/LTTq+/vvmp+\nUW03Iy7Lzm3CmLjZHbebeA4Lu+NxclMMuhrslPDDCav7ptNk0itJgltiEEOohO26NJDOFLhgLln5\nTMnbNydacCp34VQn0XAndeq7GamkL3fQ4ZFRrN3YV7bV1c1V6ERbqTcWDcyWzlq40FkTedQiRqma\nLrFB619Wiw5jYQTuz10zYcXw1cpSlvXx/Kh1kh4zftrR0hjHR8/VrKV2fZqIS0nlKXIKmNzYUBCe\nsWcw7ZngxBzCUq9QtJHQqGSMll9xU+k4MWtiirAEmR/srCvWGlTGfs2/Kx3fZ/SF0/3S6Hun9gD2\n7pfmGDnrcWdyyvPhWu4DzmnM2J3Xp17eE2iy1dTgL/spYC9aSzm2ZCKGv/voAke3v90D6Yqt8Bvu\nYItmtwdaQHETt0+9vMf2b7zqJN65dnNh4hi4JxCyYsRgntvZWjFLjJNYsMaReglRN5KJGEZGVSBR\nEhPg9GkteHP/cL6uIKAQRlZws3uq34m6mWQihtGcKqtGmTXOcDwnlTBc3AEEzgybiEnFLMFGDVID\nPwsp5pqI5Wb4jPIpTyZika6/ZiXV3Ihnt+13HCstuvAq5f41kM4UlBXxSnpkLBDftroHq9Ztr1qu\ng7CheyQJjSBujFFqQxCXQ4Nl3R14fsXFeGvl5Xh+xcVVufi9Jrt2Ln3WdgIIfKxuGA9Up5uunQXS\ncG0zkkms33EITQ1jt6K25kSBoHE6biMhhBnD/taRSuICUy2hoFgnDl4E9dcP4rLrJNaD2hrvu/oc\nLOvuQEtj3Pbz1mSwQHFr33thFDx3EsOpZKLIDSqnxsaQNX7Sqa3W8WK+vrvueRqrX+otypx5zXma\nEPc7eUhnsnjxzX5f3w1KMhHH4tPais6vMck2ruflS+fmkzxo4sluWzFbq3QiLrjv6nPwxn0f9t2u\nJae34x+u78Ku/mN5MZNV4Qg2wD5rbkuT/3XldCaHlsaGvCtuKpko+L+/bRRel2G4YUcBYwQY48Ts\nhr2suwMdARY125oTWHXdQsf7SLk0N8Zx2+qe/PPJayHFfK9e1t2Bnrsuw02LO8P0xosMlRZsyUS4\nkmD3QNp13jKYzuCiedNK2rb1nnfvsgWOY9LIXGm4sBqL3OXOf2oBLW0kNKpl2QmzDZVMXBI2ftyx\n3G6QlThWtwdqXCQ/GXZrg3V1zLra7XTchvXF6VwvWflMScdkLl7uhNXSWkpmLHMyFLvsm8Z7jqIV\nWh+4lcYwY1iIRxzKAmSyzhMCp752S/lth1Hw3GopNATJqnXbi1bJ7ZLSeMWcAfZ1texW4BW0EgJA\nMHe4sKwwdtazxzf0FVlSz+1sdUxE49QWo7yFuR+0pAnx/IqzX1544xB+u+dIxVzpWm2EVVC3duMY\nrW6OADD/iz/xlSnR2OeN/+eXkczeGQMQZOpuV+jcShB31GOZHNbvOOR4HymHRFzy58hw//e6yhTG\n7gfG+b532QIsmt2ev1+1JhMYGc1WRPTERItHK8fCGxXCLrNh3IsdM/ECJWfbNN/zjOex0/WtUJzp\n1G/ZqKjhWVxbRL4F4AoA+5RS73b53nsAvAjgBqXUY147ZnFtEgWCFgatJUHcROyoxLF6FUottWC3\nuU1+CoGX0jan/ZoLkPstTF5KAWUrTkXkJyRitpNHuyLsTpMc47ulFil/wKGunVNf+HGdMoSbuc/d\nztnbKy/P/9/tew/c0AUgeGxe0LpYYcQ72Y1jp3NkLCYEcYe0GyNRTbeeiAtWXbvQV1/4xXCvTSUT\nOHws46uobyqZwBULp9c0dbvT/aQ5EdOS1Pi8zwjguyZl0CLodhgCsZSi92FcT3ZivZS2uNEYF7Q0\naXHYRpuTiRiOj+bKKhodBrWKeUslExgaGS16dhmlK4K63wbBrd6lFwLgLdNzpZb4La7txxb6EIAP\neewsDuDvAazz1TpCIkIlE5eEjTmLI1DsHuflBlqJY/WKFbS6G/ndl/l7pWTZdGtbRyqJmxZ35t0r\n4iK4aXEn3ja5uprdPq3uFE4xdi2NDQVtvGlxZyC3I7vJWDqTtXUDtTvXy7o7cOPizqLtJuKS/26p\n53rVuu22riR252bVdQuxxIdrqiHYzO7FbuPpzrWb8/93c7Fc1t1RUqr2IOIgmYjj4++b5fodJ/cs\n0X+cxrGbZfXhF3f6bqfdGKlkbbtyyWSLs0jaubsHwZhED6T9CTbjuw9XQLB55U6yXkOrrl1Y8N4D\nN3ShraUpkGDz8hgwMArVlyuaBvWEMkH7TxCO5dqINzVnwg1bfI9kFS4/ZzqSiXi+zelMZQRbEBfP\njlQSNy7uLOt6KQWBlmTMOl6rlfDDS7C59WE9pv73dI9USj0nInM8vva/ADwO4D0htImQqlHpxCVh\nY63VZc2Ot2rddty2usfWLbQSx+rHrcZPQXGvNpWS1MUte+ay7g7bumQGbuULnCbVg5bA6LUb+/Ju\nd+Vg1AXz4/K7aHZ7cSZL039LzXjo5kprd278ut1Z+3L50rmO1kKjYHOHyYXQeo5EtH4PO0FIMhFD\ne0tTUf+7JaBRsHdla7CxKJlxLewdoM0TbOJTwliM8lrN96pn6YY1i6Txu9yaeEEJe1eGFdHLndhq\nFbOOkdt8WizsLE5uybPCEvMzUknc8+SWwP1nxOiG0e9mt7cwSv3Y4VcIBjkmwyps4Hafs2N4ZBQP\nv7gTrclEVRdmDPdUp9j+Sp0DP7j1f72m/i87pk1EOgB8FMDF8BBtInILgFsAoLOzeEWYkLDwk90R\nqHyB60riVWzZOsmuxLGaH/puqdQN/Ii8RExC6f9yYizdrJJ+xG+YbmhGXTA/7V61bnuRa2Imp/KT\nmHJSqdvFADhdZ37FgZ0493KlMQqJX3NeB364aU9Rnavlj24KcFT+OJbJ2boQX36Osxtdh0OcYSar\ncPuaTY6LK27CNQiG1QEYuxacxm4qmcDQ8VF/RZM9Pi8ncx+AojG2rLvDt1iJIlYB5XT9+Ykx9rvo\n0tzY4BlLfOvqHtzz5BbcdeX8UMR8MhHHRfOmlWzZClO4GX3kdVyC0hey/OD3WNxCFBbNbsftazZ5\nWiKN+0y5118puPVzrbyW3Nxt/ZbaiSJhpIp5AMDnlVKeswCl1INKqUVKqUXTppWWMYYQL9xc26yU\n6nrnpw1hZmn0wk9R8Uodq5Gh8oEbugIXFE8lE8VFRUNM+1Vqlk+3wuR+MpR6rVwnYlJUHNTuPet2\nvcaVlwus0f9u2fSMc+O2HaMtTteZH+ttIiYYHhktOhY/LqXpTBbPbttvm12wEqnI7Y7HKB1gh0Dr\nDycrXFapfJ/dtrqnwPXTyc21FNKZLG5fsynfxxfNm2Y7du++aj5WXbewLPfesLAbw1H0fHDK2GlF\nqbEkQMZ9welvzfdsu2vdr7uotQ+d7keGsE+VmSXTqH9VrmdBKVlx7RDA133IeC7Usni318Lpsu4O\nT1fsWuPWz36vXXMG6XJxc7cVoGoZvytBGL20CMD3RORtANcC+LqILAthu4SUhB8BYybs9P1BRGNY\n+I1Xq2SpAr+i0NyGlqaGoqKihiWiWoLXDjdh5uc43VYXnWJW7N4zb9fPuHITmwZuabGNY/SzHbfr\nzK7/EjHJp2BP6gkV7FIwL18619fEzSudtBOi5XhAKpkoEsl22E2qjLgdJ3kYRDYasWrm8+jmuhsU\ns0A0LJR2Y8xajuPZbft9n4swsRt7tWiHG8lEHPdfv9CXqDVcPs2lUdysJn0DaXTd8zRuXd1TdK0D\nwH1XL/AUjNY+dLtOnOJm/WL0hdH2coiL+L523HrAcNlzE0NmoXb3VfNtS2OUun+/+F04DcPNvlJ4\niU6/Cw3JRNz2HDQnYr7u0wZGLKfTtRnFBaAglO0eqZQ61fi/iDwE4IdKqbXlbpeQUql1chG3yWyl\nVneiEpsXNPbM6ZwYk5palWDwcq30Ok63MgVmNxi7bTht18+4CuICa02LbT1Gr+24XWdu/WdkubRi\nHMvzKy7G+h2HXEUR4J1O2omG2FhMmWH9cNqGYUEAUFCeYXhkNNS4J6OGnbm/WhrjvlLUB8GwUNq5\nYjm5WJdSzsKKALjg9Hb8+q1+V0uo01hd1t3ha0w4EUZmQjNGvODypXOx/NFNntbdIDFjAnsXN/P1\nATi7Wdr1oZcLoF+XOoFWmkFEq01pLlVi3KdLJZmIB3Lbbk0mcHw05/g3xv3JqURCJpvDqSueyh9P\nJqfy4ySlv+c07hNxwQ3vmVXknh0Ew9PArtiz1e28Uu6b5WIXO2nF+Myr6PlAOlPsfRIXNCXiGHa5\n/wiAVHOiYDw6ZXmul9AXNzxFm4g8AuBCAFNFZBeAuwAkAEAp9W8VbR0hJVBrAROmaBzvsXl+Hki1\nqqdSSvITg0qcDz/jKmgcn9Mx+tmO13XmtO1V67Y7TryNYzELyr6BtG1dN/NkMUj5hUx2LMbP+HEq\nIZDTJ/pWMVMKAiDmIh6MGnal7iPlMYk12D2QLrivtLpMUNOZLJoaYkUTaq/Yo0RMMHFCg+1Eypo8\nySgQ7jVW7RYZzH/v1p4wBRug9dXn1vSgNZnw5Y7r93x69avZzRkojiW2q40JaPejctOue5WTKSVO\n1lxnMmjdRyNBk1P8Z2sygVXrtjvWtDMWRMxCQkvfr7kLmwWUWXCYhcqi2e0l96vhaQAULk4Cxfeb\nWqXy98IaO+mEcZ9du7HPMT4vLsW17jJZ5blgpPR2DFi+V05Me5TxrNNWKVinjVSKUut6hUVY9dCC\nHodfgRcl/CbsqFQ9lUr2WdjbjlpNQbtzZ7imuLn3udVYczoWt760+wxwT45jHU9ufQuU7/ZlHJdb\nLb1yMOpxAe7HDfgXd9ZtW8WSNaudXd29IJR7vZRb062SOFn6rIXVvdpvvT6MuErrgobdM6L7S0+X\nbDH1en6WWhPTeq0HuT6Mv3c6rma9flcp15pT27yuATOlCC23+411e+aFEav1c86UJF58sz/0xQq7\nNnk9l/30W1Arq7UNfsZ/1PFbp42ijYxLailgwhKNUZukVwrzuXKyRFTimJ0KQ9tZCKJArRcj7Agy\naTRwKx7ttxCwX/xeQ25963cSabgA/mbnoOs5suuzMHjbhwg1XImCTN5LEdJBcet/wN9qedCMrdbJ\nnvHa6f1ysU5MgxRWt/u+m8BxEh2lZI6Ni+D+651LVHi12w63e4Sf68P8926Csa1E116rGAnad8lE\nHNec15G3ArtZ2K37BZzHm9Uy6XZOSj3fdoWyna4Br+ey03Vt7hsvK2spJUSsiyFBLPq1wq9oKzum\njZAoUo5rWxj7Bko3y3vF2USx8Hc5uJUuACrn5ulUJNvOZSUKN/gouns8u21/0cPcy53VznU0SCHg\nIPh1U3XBWqrfAAAgAElEQVTrW7fJhHli4OQCaD1Hdm5+dqUBAP8Cwhp071ag2+pG5IbbtRfmPdYp\nXtMqSpyuSWtmxqxSjiUXgDFroPU82Z3rMASb4bboNXF0Kslhdsnzej4AxVleza6wExKx/KKUU/8Y\n+F0Ucmp3PCZFiaa84qC83GCtfeda11CVZsWxhlIErWNn9Jn5XPkR/25xun6sf+XW3hMAPXdd5ts6\n5vVcdrqu7eJq1+84ZFsu4oqF013rYdphdTU3bzdqz/Wg0NJGSITwszo23ixtVqplJfXr0hPl/q6m\nRdluX04r/V5uM7Vut9e+rBNd68pz2BZOvyvSTpMnvxabIO6e1axlFNS9znxNelnpglinS3Hz84vf\nMePl7utn8m92x3U7frfnTVwEH3/fLN+ZTO0sZNXwXFi7sc81ruyBG7o8k2CYMfrXLP6DWBHd+t7Y\ndlJ33TQTdMx6nVu3sVyK5ayU+6hTG+yeD273LKfFtwmJWMkuv1F7rtPSRkgd4rU6Vq3kIrWM9aqW\nldRvVq6oWjb9FFSv9L6cXO28kv5U0xLud19Oq+ID6Uy+XEGlJp9BrKhuGT8NvCyMbpPcWrjcBs2Q\nZ74m3bKqGpMyv/eyMIqPO+E3oZJ1vFqvPa8WCFAg9NwyzloTmpjHfVYpPL6hD4tmt+e/49aHdlb3\nTE6hubEBG794mWN77ZLiWDNTej0vPremB3anJi6SP04nt8vmRAxNiTj6hzNFxw8ETwRi1J60c4k0\nXh2zCDYBihLIePW307m9+4ktWNbd4XpNGSVXrAtRXvXigt4TgiSF85ON2Cy+JyRiuPyc6a5xhW5E\n9bnuBUUbIRHCq75XNVa+KykGqik0vHBy6bES1bou1Swt4bQvu8yCdgXBo+TSaYfXxNjP5LNcrG7C\nq9Ztx22rewKXmzC+AzhP/O55cout2DZKHJRyfso5z36vRQPzNemnoLxVBJnLN5jb6SR2775qPgA4\nZr7zSykTxaDlAsxuxn4zzi7r7rC1dBgiwJy4xumeXUrWZOt1Z7aG9Q2ksfzRTQWZYJ327aSlzefK\nq8SJW1yeXayjE8Z15TZOrJ8oFNZi83ONO/WrURfQ7ZoazuQqvhAFBMui7CXw1u84hEHT+OgfzuTr\nTpq9Ebxcfq3brTco2giJEH7re1WSSoqBWtSwc8I6sXVyg6uEZTMMIVPNeoRO2zTSbrtldoyCSPfq\nbz8T42qtzIbVZ24Tv7uunB9qUpty22y9Ft0SN1ivySCr+V7ttFqe4iIFBePvv36hp7h0c9sqZaLo\nd9zZxYqFYelwqxkXZF9216DXdWdn2bTbt5MLozXW0+2a8OpnIzOq8b2w3Wj9nGdzH7phtTLb9U0m\np3A4PVpy8qcg5TvKLVu0dmOfrZXULj7OT4hJPZRDcoKijZAIEYV6a5UUA7UufG7FbhW+0lahsCbl\n1axH6LSvmGj5zqwPTcOSYTf5rrZI99PffsZftVZmq7GwEXZSmzDa7JWQCLAXJkHumX7aafy2GzP3\nXb0A9129wHOyav17tzZ54dd11OpyB4Rj6XDCes1cNG+abSKJi+ZNc7wGS03zbt13GM9Nr+O3xlGW\nW/fObv9uBMkGaa3F6ERWqZKePXbn0y3ZR5CFG6sLJOCvtqd1O6XWg4w6FG2ERIiwJ1Ol4PTwak0m\nHN2Kyt12VFwVqhFrFdakvJoC38nVxvrQtz7Mnawl1RTpfvrba8JWzYWTai1s+HXH9EPYbQ5yHwzy\nXb/tdIsXsssY6oSdxc7cZj/4dR21u4eEYenwazU0u/dZ3392237b/iwV677DeG56FSA3X//Lujt8\nizanGn1W9gymMWfFU45hEEGzQfptXynPHj9tKXWh6fjo2OJD/3DGc+zbzR2qGTNdbSjaCIkYtb7h\n2D28EzHB0MhoQRrdUlboKi006iF+KqwJbjUFvrFNu5ge88PZ78SimiLdT387lSEop1h0qVR7YSMM\ny28l2hzkPmi9FpzEkd92urkK+r0HulnsnP7Gz7G5LTDYvR/U0mGXudLPPdupz8opeJ6ISUFMm9O+\njfaXc40u6+5wjPVMJRNF2/aTVdItI6QVwxPUGCPrdxwqsA4F6cegrptWy5zXs8TvsyroM81pscRJ\n+JqT7pwoULQRQgqwe3jbBfeWspJWSaGxdmMflj+6KR8HkQ9kRzBhWWnCnOBWU+Av6+7AbQ6rt8bD\n2c9Dutruvn76OwoWboNqu0iHYfmttVu3X+Hpt51+J8le/RSWVd16nbsVT1+7sa/kcet2P/G6Npz6\nzE8Cj7gIckqVlD0yTJxiPY1ENGacFjfdShz4qc0IaGPEHMMVNINlUFqTiUCLC36vj6DPNKfnR1ap\nooRXlartGXUo2gghRVgf3qeueMr2e6W4P1VKaNz9xJaiwPVMTuVTIEeFWk9wy8FLADl9bkzKaiGG\nghTYjsI4qbaADMPyW2vR61cc+W1nkGyWbv0UxCIWBKcaiQooO/bRyeLitU03a7UXOaVc6zpWc2EK\nKM0t105wWseen2eqgd25LVe4pZKJgkyggHYvFCl2V3VbXPBzfZTyTHNLxGYkrqn1olqtoWgjhHgS\n9Vg0wD7Lmdv7taLWE9xy8BJATp9Xu+6XmXrs72oKyLCu7XLaXK5bsx/had2HW9Y8v94GgHs/Obl1\nxfUEPqXiFlNVTuxjOa6yQdw4raSaE47tqfZ16zWOnYqfB+03pxqXbhju2qWIfrPF0Np+Nw8Kt3MQ\nJNmHn3PpZL0cHhnNx9tar9t6CIkIE4o2Qogn9WwdiiJRseoExUsARVUgRbW/ozDhqPW1XY2YulL2\nYR0zdtn7vPrJKQFFObXeDJwm7+UspJXrzunXjdOKXXdUs1yI3+vQrk12Fk/Au9+OuVipnCxqRgbL\nIJkkgeKsq3ZJTpySj/kpkeGF33NpZ70cMi2WWP8uKiVlqglFGyHEk6hOxs20Oaxctjms4pLS8HpY\nR1UgRY2oTDhqfW1XI6YurJIExrb89lMl70nliG0nkRJ2FlC/bqaDNt4Q1arpGeQ6tGuTm/x267e0\nTYkGgxsXd+LxDX2O5zaIJTiVTGDjFy8ret88BlqTCSTiUpTwxctt0q/YDXIuzc+PJSufKfKU8Up8\nVe2SMtWGoo0Q4ouoT8bvunI+lj+2qeDBk4gL7rqyOIickFoTpQlHLa/tasTUhZmxNUg/ORnUQjC0\nlSy23URK2G7w5ja6WdyCFPsOu/RFkOsw6L6tZXLMLoRu3LtsARbNbnc9t34twXdfNd+2ELZZFA6k\nM0jEBG3NiYKYPC+3Sb9it9Rz6fV3Uav7Wg0o2ggh44JaWwwIMfCzAn0iTjjsCEMoePV3rWJy7SxI\nTu+X4ipbith2EymVsN55kYhLoGLfYZ+zINdhkDg9uzI5dsXHrRhW2KDn1un5BxTH25kzUxpkcgrN\njQ0FVjknsT0jlQwkdks9l6UmvopSrH3YxGrdAEIICYtl3R14fsXFeGvl5Xh+xcUUbKTqGCvQfQNp\nKIytQK/d2FfwPaeJxXiecNixfOlcJBPxgveCxNT56W+3fazd2IclK5/BnBVP4fQ7foQ5K57CkpXP\nFJ2vUvB7jv2OmTBwy2i5rLsD9129AB2pJARaDJWfJEJO7b9z7eb8+3a0NSew6tqFjhkKredM9G2H\ndX6AYNehXZvs6EglMXFCQ4HXhx/K9Qyxe/4Fcens0y1oBk7n4KJ50wKJ3YvmTbP9rtP7bvu3Jr4q\n595Rj1C0EUIIISHhtgJt5kSccNhRqlAw8NPfTvsAUCAqjAQhYYkmp0n+8Mhowbb9jpkwcMpcabxf\nysKXU/u/++JO23i2jlQSb6+8HBu/eJlrIhjjnAGFyTnCFLVBrkOjTU59KAAeuKELz6+4GAMBMkMa\nY9JJwJZDUMu9uV+XdXfgmvM6YD5aBeDxDX2OGT/txO6z2/bbftfpfQO3e4Nh2TWKbwPB7x31CN0j\nCSHjiihk5CPhUW/n0+8KNN15xygnpi5If1v3sWTlM45JMsKILzT+9u4nthQkVOgfzhTE/1TKVdaa\nbEKkMhktg7bT7/eNc2aXhTKs+M+g1+Gy7g7HWC9l2p5fV0ojI2RQ1m7sKxhX1gyRBkGLnlv79dlt\n+4u+l85k0dQQKyp47SR2yxnfdrF7Xfc8XXA9GcW3T4T7Jy1thJBxQzXdjEjlsTuft67uQfeXno7s\nOQ3ibkV33vIpx8201EQIQVjW3YGWpuL1cbMlrRKustZrZyCdca0L1lHGvoK2M+j3Kx3/GfQ6dGq/\nuQ/9uFKWallfu7EPyx/dVLQQsPyxTUX3RSdL4o2LOx23b+5Xpz4eTGd8WcjXbuxDzMEyGXQcGGPa\nrvZqpSzTUYOijRAybqimmxGpPHbnExizVERRuNHtsbqU099+EiGEgZfoqMSYcbp27Ch3X35jvUrd\nV9TiP/2cLzvXvpsWd5bsBmxm1brtyOSK7WSZrCp61jm5GN67bIGjUDf3q1vfe4ldQ2TZWXFLGQde\nY/pESOJE0UYIGTcwI9/4wr3OUTTFeLkxWiQY5fS3m9gIU2h7iY5KjBm/97ww9mVtv1vcXCn7Wr50\nLhKxwm0mYvZZJ6uB3/NliJqv3dAFAHhYzx75NT3uLWyXYKfPnMSVnwQh5SwoOImsUseB15g+EZI4\nMaaNEDJuOBFTAI9nvOJCoirGo17TcLzhp7/dYiON1OZxEWSVQkfI8YV+UumHPWb8xFSVGk9lh7n9\nTjXDyhKHVh1orwurhtv5ssYSDo2M5jNJutUz84vbuQ3yrPOTIKSc2Fun+3NOqZKO3e24TxRvBoo2\nQsi4oZw6Q+ONekvgYYfd+TRT72J8PJyjesCrEHCl+7wWSWe8rp1K3hfDPt5V67YXpc83XAGjdr1Y\nx5pb/FWpbV++dC6WP7qpyEXSqeadE+Uk8fGD1yJq0Puf05h2SsIyHqFoI4SMG5iRT8NrklovOGXf\nA+pfjI+Xc1QPBCkEXCmqbX213guN7JEDw5mq3BeDHK/X5L2e3N79xhIa9eZKOQ9298VShEulPVPc\nFlFLuf/x+Q6IKiPNazksWrRIrV+/vib7JoSQ8YxdimwgXHeoajPerFLj8RxFlVNXPGWb3lwAvLXy\n8mo3h5jw40pZT9eK01hzomy30TKoiBurzT7s7tvlntPx9jwQkQ1KqUVe36OljRBCxhn1tDLtl/EW\nJzYez1FUYaxrdPFjBa0nt3e/9dkMqm3xNRO25cpJSNltr5z734nspcDskYQQMs6IWopsUgzPUfVg\nGYbo4mfyXk8ZWe3GWiImaGtOOP5NLRdqwqoVGbRGajn3vxO5tA9FGyGEjDM4SY0+PEfVo54m/Sca\nfifv9VKI3m6srbpuITZ+8TJfddHqlaBCqpz734nspUD3SEIIGWcwYDv68BxVl/HmXjteqCfXR784\njbXxeKwGQYVUOfe/E9ndmaKNEELGIZykRh+eI3KicyItXoznYy1FSJV6/xvP4tcLZo8khBBCCCGE\nlEQ1MlFa9zeexC+zRxJCCCGEEEIqSrWtiCeqlwJFGyGEEEIIIaRkTlQhVU2YPZIQQgghhBBCIoyn\naBORb4nIPhF5xeHzG0XkZf3nBRFZGH4zCSGEEEIIIeTExI+l7SEAH3L5/C0Av6OUOgfAlwE8GEK7\nCCGEEEIIIYTAR0ybUuo5EZnj8vkLppcvAphZfrMIIYQQQgghhADhx7R9GsCPnT4UkVtEZL2IrN+/\nf3/IuyaEEEIIIYSQ8Udook1ELoIm2j7v9B2l1INKqUVKqUXTpk0La9eEEEIIIYQQMm4JJeW/iJwD\n4JsAfk8pdTCMbRJCCCGEEEIICcHSJiKdAL4P4JNKqVfLbxIhhBBCCCGEEANRSrl/QeQRABcCmArg\nHQB3AUgAgFLq30TkmwCuAbBD/5NRpdQizx2L7Df9TZSYCuBArRtBThg43ki14Fgj1YJjjVQTjjdS\nLSo11mYrpTzjxjxF24mGiKz3IzoJCQOON1ItONZIteBYI9WE441Ui1qPtbCzRxJCCCGEEEIICRGK\nNkIIIYQQQgiJMBRtxTxY6waQEwqON1ItONZIteBYI9WE441Ui5qONca0EUIIIYQQQkiEoaWNEEII\nIYQQQiIMRRshhBBCCCGERBiKNhMi8iER2S4ir4vIilq3h9QfIvItEdknIq+Y3msXkZ+KyGv67zb9\nfRGRf9LH28sicq7pbz6lf/81EflULY6FRBsRmSUiz4rIVhHZIiKf1d/neCOhIiITROTXIrJJH2v3\n6O+fKiK/0sfNahFp1N9v0l+/rn8+x7StO/T3t4vI0tocEYk6IhIXkY0i8kP9NccaqQgi8raIbBaR\nHhFZr78XyecoRZuOiMQB/CuA3wNwNoCPi8jZtW0VqUMeAvAhy3srAPxMKXUmgJ/prwFtrJ2p/9wC\n4BuAdrOAVsT+fQDeC+Au44ZBiIlRALcrpc4CsBjAn+v3LI43EjbHAVyslFoIoAvAh0RkMYC/B/A1\nfaz1A/i0/v1PA+hXSp0B4Gv696CPz48BmA/tPvl1/dlLiJXPAthqes2xRirJRUqpLlMNtkg+Ryna\nxngvgNeVUm8qpUYAfA/AR2rcJlJnKKWeA3DI8vZHAHxb//+3ASwzvf//lMaLAFIiMh3AUgA/VUod\nUkr1A/gpioUgOcFRSu1RSv1G//8RaBOcDnC8kZDRx8xR/WVC/1EALgbwmP6+dawZY/AxAJeIiOjv\nf08pdVwp9RaA16E9ewnJIyIzAVwO4Jv6awHHGqkukXyOUrSN0QGg1/R6l/4eIeVyslJqD6BNtAGc\npL/vNOY4FkkgdJegbgC/AscbqQC6u1oPgH3QJiRvABhQSo3qXzGPm/yY0j8fBDAFHGvEHw8A+GsA\nOf31FHCskcqhADwtIhtE5Bb9vUg+RxvC3mAdIzbvsR4CqSROY45jkfhGRCYCeBzArUqpw9ois/1X\nbd7jeCO+UEplAXSJSArADwCcZfc1/TfHGikJEbkCwD6l1AYRudB42+arHGskLJYopXaLyEkAfioi\n21y+W9PxRkvbGLsAzDK9nglgd43aQsYX7+jmc+i/9+nvO405jkXiCxFJQBNsDyulvq+/zfFGKoZS\nagDAz6HFUaZExFj8NY+b/JjSP2+F5jbOsUa8WALgKhF5G1qYysXQLG8ca6QiKKV267/3QVuQei8i\n+hylaBvjJQBn6hmKGqEFsD5R4zaR8cETAIxMQp8C8J+m939fz0a0GMCgboZfB+AyEWnTA1kv098j\nJI8et/F/AWxVSv2D6SOONxIqIjJNt7BBRJIALoUWQ/ksgGv1r1nHmjEGrwXwjFJK6e9/TM/4dyq0\nYP5fV+coSD2glLpDKTVTKTUH2jzsGaXUjeBYIxVARFpEZJLxf2jPv1cQ0eco3SN1lFKjIvIX0Do5\nDuBbSqktNW4WqTNE5BEAFwKYKiK7oGUTWglgjYh8GsBOANfpX/8RgA9DC5AeBvAHAKCUOiQiX4a2\nkAAAX1JKWZObELIEwCcBbNZjjQDgC+B4I+EzHcC39ex7MQBrlFI/FJHfAvieiNwLYCO0RQTov78j\nIq9Ds3p8DACUUltEZA2A30LLfvrnutslIV58HhxrJHxOBvADPaygAcB/KKV+IiIvIYLPUdEWJAgh\nhBBCCCGERBG6RxJCCCGEEEJIhKFoI4QQQgghhJAIQ9FGCCGEEEIIIRGGoo0QQgghhBBCIgxFGyGE\nEEIIIYREGIo2QgghdYOIHNV/zxGRT4S87S9YXr8Q5vYJIYSQUqFoI4QQUo/MARBItOl1xtwoEG1K\nqQsCtokQQgipCBRthBBC6pGVAD4gIj0icpuIxEVklYi8JCIvi8ifAICIXCgiz4rIfwDYrL+3VkQ2\niMgWEblFf28lgKS+vYf19wyrnujbfkVENovIDaZt/1xEHhORbSLysOhVWgkhhJAwaah1AwghhJAS\nWAHgr5RSVwCALr4GlVLvEZEmAM+LyNP6d98L4N1Kqbf013+olDokIkkAL4nI40qpFSLyF0qpLpt9\nXQ2gC8BCAFP1v3lO/6wbwHwAuwE8D2AJgF+Ef7iEEEJOZGhpI4QQMh64DMDvi0gPgF8BmALgTP2z\nX5sEGwD8pYhsAvAigFmm7znxfgCPKKWySql3APw3gPeYtr1LKZUD0APNbZMQQggJFVraCCGEjAcE\nwP9SSq0reFPkQgBDlteXAjhfKTUsIj8HMMHHtp04bvp/FnyuEkIIqQC0tBFCCKlHjgCYZHq9DsBn\nRCQBACLyLhFpsfm7VgD9umCbB2Cx6bOM8fcWngNwgx43Nw3ABwH8OpSjIIQQQnzAFUFCCCH1yMsA\nRnU3x4cA/CM018Tf6MlA9gNYZvN3PwHwpyLyMoDt0FwkDR4E8LKI/EYpdaPp/R8AOB/AJgAKwF8r\npfbqoo8QQgipOKKUqnUbCCGEEEIIIYQ4QPdIQgghhBBCCIkwFG2EEEIIIYQQEmEo2gghhBBCCCEk\nwlC0EUIIIYQQQkiEoWgjhBBCCCGEkAhD0UYIIYQQQgghEYaijRBCCCGEEEIiDEUbIYQQQgghhEQY\nijZCCCGEEEIIiTAUbYQQQgghhBASYSjaCCGEEEIIISTCULQRQgghhBBCSIShaCOEEEIIIYSQCEPR\nRgghhBBCCCERhqKNEEJIJBGRn4tIv4g01bothBBCSC2haCOEEBI5RGQOgA8AUACuquJ+G6q1L0II\nIcQvFG2EEEKiyO8DeBHAQwA+ZbwpIkkRuV9EdojIoIj8QkSS+mfvF5EXRGRARHpF5Gb9/Z+LyB+Z\ntnGziPzC9FqJyJ+LyGsAXtPf+0d9G4dFZIOIfMD0/biIfEFE3hCRI/rns0TkX0XkfvNBiMiTInJr\nJTqIEELIiQNFGyGEkCjy+wAe1n+WisjJ+vtfBXAegAsAtAP4awA5EekE8GMA/wxgGoAuAD0B9rcM\nwPsAnK2/fknfRjuA/wDwqIhM0D/7HICPA/gwgMkA/hDAMIBvA/i4iMQAQESmArgEwCNBDpwQQgix\nQtFGCCEkUojI+wHMBrBGKbUBwBsAPqGLoT8E8FmlVJ9SKquUekEpdRzAjQD+Syn1iFIqo5Q6qJQK\nItruU0odUkqlAUAp9V19G6NKqfsBNAGYq3/3jwDcqZTarjQ26d/9NYBBaEINAD4G4OdKqXfK7BJC\nCCEnOBRthBBCosanADytlDqgv/4P/b2pACZAE3FWZjm875de8wsRuV1EtuoumAMAWvX9e+3r2wBu\n0v9/E4DvlNEmQgghBADAgGtCCCGRQY9Pux5AXET26m83AUgBmA7gGIDTAWyy/GkvgPc6bHYIQLPp\n9Sk231GmNnwAwOehWcy2KKVyItIPQEz7Oh3AKzbb+S6AV0RkIYCzAKx1aBMhhBDiG1raCCGERIll\nALLQYsu69J+zAPwPtDi3bwH4BxGZoScEOV8vCfAwgEtF5HoRaRCRKSLSpW+zB8DVItIsImcA+LRH\nGyYBGAWwH0CDiHwRWuyawTcBfFlEzhSNc0RkCgAopXZBi4f7DoDHDXdLQgghpBwo2gghhESJTwH4\n/5RSO5VSe40fAP8CLW5tBYDN0ITRIQB/DyCmlNoJLTHI7fr7PQAW6tv8GoARAO9Ac1982KMN66Al\nNXkVwA5o1j2z++Q/AFgD4GkAhwH8XwBJ0+ffBrAAdI0khBASEqKU8v4WIYQQQnwhIh+E5iY5RymV\nq3V7CCGE1D+0tBFCCCEhISIJAJ8F8E0KNkIIIWFB0UYIIYSEgIicBWAAWsKUB2rcHEIIIeMIukcS\nQgghhBBCSIShpY0QQgghhBBCIkzN6rRNnTpVzZkzp1a7J4QQQgghhJCasmHDhgNKqWle36uZaJsz\nZw7Wr19fq90TQgghhBBCSE0RkR1+vkf3SEIIIYQQQgiJMBRthBBCCCGEEBJhKNoIIYQQQgghJMJQ\ntBFCCCGEEEJIhKFoI4QQQgghhJAIQ9FGCCGEEEIIIRGGoo0QQgghhBBCIgxFGyGEEEIIIYREGIo2\nQgghhBBCCIkwDbVuACGEEEIIIYRUgrUb+7Bq3XbsHkhjRiqJ5UvnYll3R62bFRiKNkIIIYQQQsi4\nY+3GPtzx/c1IZ7IAgL6BNO74/mYAqDvhRtFGCCGEEEIIqRtyOYXDxzI4NDSC/uERHBrKoH9oBAfz\nr0fQPzSC517bj0xWFfxtOpPFqnXbKdoIIYQQQgghxA9KKQyPZE0CbEyIHRo6nhdkh4Y1IdY/PIL+\n4QyyOWW7vcaGGKa0NKK9pbFIsBnsHkhX8pAqAkUbIYQQQgghJBRGRnMF1i5DbB3Mv9ZFmEmkHR/N\n2W4rHhO0NSfQ1qyJsDNOmoi2lka0Nzdqv1sSaG9p0l8n0N7SiGQiDhEBACxZ+Qz6bATajFSyon1Q\nCSjaCCGEEEIIIUVkcwqD6UyhFaxAgBUKsf6hERw5Puq4vckTGtCuW8FmpCZg/ozJaG9ptAgx/ae5\nEZMmNCAWk5Lbv3zp3IKYNgBIJuJYvnRuydusFRRthBBCCCGE1AmlZkNUSuHo8VH0D2XGxJYuxvIi\nrMBFMYOB4RE4eCEimYjrgkuzdp06pdlefLU0oq25EanmBBLx6lYbM/plPGSPFKUczkSFWbRokVq/\nfn1N9k0IIYQQQki9Yc2GCABNDTH84ZJTcdaMyfZWMCMObCiDkay9G2JDTPKCKy+0WhIFAqytuVCE\nJRvj1TrscY2IbFBKLfL6Hi1thBBCCCGERJhjmSy27B7EXU+8UiDYAOD4aA7f+O83Ct5LNY8Jrlnt\nzVg4M5WPATOLL8M1cVJTQz4OjEQTijZCCCGEEEIiglIKu/rT2Ng7gN/s6MfG3gH8dvegYyZEABAA\nP/3cB9HW3IjWZAINVXZDJJWHoo0QQgghhJAakR7J4uVdAwUibf+R4wCACYkYzpmZwqfffxq6O1O4\n6z+3YO/hY0XbmJFK4oyTJlW76aSKULQRQgghhBBSBZRS2HFwGBt7+/GbHQPY2NuPrXuO5GuOzZnS\njEMBo/AAACAASURBVPefMRXndqbQ3dmGuadMKkjekR7JjptsiCQYFG2EEEIIISFSanY/Mv44enwU\nL/cO4Dc7+7Fxp2ZNOzQ0AgBoaYxj4awUPvM7p6NbF2ntLY2u2xtP2RBJMCjaCCGEEEJCwprdr28g\njTu+vxkAOLEe5+RyCm8eGBoTaDv78eo7R/Ip80+f1oJL5p2E7s42nDs7hTNPmoR4CTXIlnV3cCyd\ngFC0EUIIIYSExKp124qy+6UzWaxat50T7XHGYDqDTSYrWk/vAAbTGQDApAkN6O5sw9L5p2hWtFlt\naG1O1LjFpJ6haCOEEEIIKZGB4RFs2jWInp0D2LRrAH0DxUkiAM3idtW//AKd7c0FP7PamzEjlSzJ\n4kKqRzan8Pq+o7pA68dvdg7g9X1HAQAiwNyTJ+HDC05B9yzNinba1ImI8ZySEKFoI4QQQgjxwbFM\nFlv3HEZP7wA29WqWlbcPDgPQJu5nTJuI5sY4hkeyRX/b3BhHazKBV/oG8ZNX9mI0N5a+vSEmmNmW\nxCxdyM2eMiboOtubMWkCLTTVpn9oBBt7DTdH7VwfPT4KAGhrTqC7sw0fWTgD585uwzkzW3mOSMWh\naCOEEEIIsZDLKbx1cChvQevpHcDWPYfztbJOmtSErlkpXP+eWeiamcK7Z7Zi8oREUUwboGX3+8pH\nF+TdI0ezOewZPIbeQ8PYqf/sODSM3kPDeGrzHgwMZwra0q4XSO5sb8Zsk4Wuc0ozTpk8gVa6MhnN\n5rBt7xFs7NXi0DbuHMBbB4YAAPGYYN4pk7CsewbO7WxDd2cb5kxpZiFqUnVEKedCfZVk0aJFav36\n9TXZNyGEEEKImf1HjhdY0DbtGsCRY5plpaUxjgUzW9E1qw1ds1qxcFYK01uTjtsqN3vkYDpTIOh2\nHhrGzoPa776BdD49PAA0xmNFVrpZJvfLliauz1vZf+S4Js70umgv7xrMi+ypExu1RCGdbejuTOGc\nma1oboxYH768BvjZl4DBXUDrTOCSLwLnXF/rVkWXiPeXiGxQSi3y/B5FGyGEEEJOJIZHRrF51yA2\n7RrApt5B9PQOoG8gDUCzrMw9eRK6OlPompnCwlkpnHHSxMhYswwr3Y6DZlE3pFnrDg7nhabB1ImN\nBSIu/zOlGSdPmjDu465GRnPYuufwmEjb2Y/eQ9q5bogJ5s+YjG5doJ3b2YaZbcloW9FeXgM8+ZdA\nJj32XiIJXPlPkRIikaEO+ouijRBCCCEnPNmcwqvvHMEm3Xq2cedAQRr2mW1JdM1KoWuWJtDePaMV\nycZ4bRtdBoPDGezQRdxO3eXSEHS7B9IwGenQ2BDDrLZkQVKU2VNa9P8no2dh8sHewWMFVrTNfYM4\nPpoDAJw8uQnnmqxo7+5oxYREnZ3rr70bGOwtfn9CCrj4zuq3J+o8cy9wbKD4/dZZwG2vVL89NvgV\nbfV3NRJCCCGE2KCUwp7BY3k3x429A3ilbzCfGKQ1mcDCWSlcdvbJWKiLtKkTm2rc6nBpbU7gnOYU\nzpmZKvosk81h90A6L+LMgu6lt/vziTYMpk5sKkqKYryeNrGp5la646NZvNJ3OB+HtnFnP3YPatk7\nG+MxvLtjMj65eHa+LpqbS2skUQrofwvY3QPs2QTs6bEXbIAmTH70V9VtXz0zuKvWLQgMRRshhBBC\n6pLDxzJ4uXcwb0HbtGsA+48cB6BN2s+eMRnXL5qFhbO0eLQTPYFEIh7D7CktmD2lBR84s/AzpRQG\nhjMFSVGMOLpfv3UIa3v6YHbOamqIaZa59sIYOiOmLmwLllIKfQNpbNw5Vhftt7sPYySrWdE6Ukmc\nN6cdfzQrhXNnt+Gs6ZPQ1FBHVrRcDjj0hi7QDJH2MnB8UPs8lgBOPhtItACZoeK/nzwDuOW56ra5\nHnjwg8Dh3cXvt86sflvKhKKNEEIIIZFnZDSHbXsP5y1om3oH8Mb+scnraVNb8IEzpmKh7uo4r94m\n7TVGRNDW0oi2lkYsnFVspRsZzaFPt9LtPDhkiqdL48U3D2LIUubgpElNRUlRjNfTJjYViGe7xC1L\n55+CzX2Dek00TaTt0wX5hEQM58xM4Q/eP0eri9aZwkmTJ1S2g8IkOwoceFUXZroFbe9mYESr+4Z4\nE3DKu4EF1wLTF2o/J50NNDQ6x2hdeg8wcVptjifKXHqPfX9d8sXatalEGNNGCCHjgHKz1RESJZRS\n2HFwuMCCtmX3YYzosUlTJzZqMWgzU+jqTOGcjhRam1knq1YopXBoaKQo06URU7fn8LECK92EREwX\nci3IjGbxwpsH86UUAK3mHRRgvDNnSnNBspC5p0xCIh6r6jGWTDYD7NtqEWivAKO6iEg0A6csAKZ3\njQm0aXOBuMt4jng2xMgR8f5iIhJCCDlBcKoLdd/VCyjcSDhUeNJzaGikwIK2addAvlZZMhHHgo7W\nvIvjwlmt6EhFPMMfKeBYJpu30vXqMXTG/7fvPQK7mejEpgb848e60DUrhSn1Enc4ehx4Z8uYONuz\nSXudHdE+b5wETD+nUKBNPROI0SJ8IhNqIhIR+RCAfwQQB/BNpdRKy+c3A1gFoE9/61+UUt8M1GJC\nCCEl8fc/2VYg2AAgncni3qd+izNOmojmxjhamhrQ3BhHc2NDZFKXkzrB6o412Ku9BkoSbscyWWzZ\nPYgePdX+pt4B7Dw0DACICfCukydh6dmnoKtTs6S96+SJaKgXqwqxZUIijtOnTcTp0yYWfXbqiqds\n/2bo+CguOevkSjetdDJpzWK2xxSDtm8rkNOTuUxo1UTZ+/5UF2hdQPtpQIxjmZSGp2gTkTiAfwXw\nuwB2AXhJRJ5QSv3W8tXVSqm/qEAbCSGEQEtdvuPgELbuOYJtew9j654j2LrnMPbo2dKsHDg6giv+\n+RdF7ycTcbQ0aQLOLOhaGhvQ3DT2e2JjA5qbGtDSGB/73diQ/9v878Y4J9XjDaWAo/uAfb8FfrS8\nMB4E0F6v+wJw2oVAyzTdn62YXE7hjf1H0WMqWL1tzxGM6nnnZ7ROwMJZKXzifZ3ompXCgo5WFoM+\nwZiRSuZr5FnfjwzHj2oxZ2YL2v7tgNIXy5LtwIwu4ILfHbOgtc1xvC4IKQU/d8b3AnhdKfUmAIjI\n9wB8BIBVtBFCCAmJweEMtu49jG17DmPbXk2cbX/nCI5ltJieeExw2tQWnDu7DUeOZXDYUlAX0OJ+\nvvLRBRgeyWJoZBTDx7XfQ8dHMTSSxbDxe2QUR46N4p3DxzB0XHs9NJLNxw/5oakhZi/+zKKwqQEt\nDqLPThw2NlROCDIG0MSxQWDfNmDfFs1SsG+r5tKVPuT+d0P7ga+eqVkUpr4LmPouHJl0Gl7LTcf6\no9Pw3P4W9PQdzaeRn9TUgHNmteKWD56Wr4tWV8kjSEVYvnSurXv38qVza9OgY4Na1kazQDvwGvIR\ndi0naQJt3hVjAq11JgUaqTh+RFsHAHNRiF0A3mfzvWtE5IMAXgVwm1LKoZBEtLnwwguL3rv++uvx\nZ3/2ZxgeHsaHP/zhos9vvvlm3HzzzThw4ACuvfbaos8/85nP4IYbbkBvby8++clPFn1+++2348or\nr8T27dvxJ3/yJ0Wf33nnnbj00kvR09ODW2+9tejzr3zlK7jgggvwwgsv4Atf+ELR5w888AC6urrw\nX//1X7j33nuLPv/3f/93zJ07F08++STuv//+os+/853vYNasWVi9ejW+8Y1vFH3+2GOPYerUqXjo\noYfw0EMPFX3+ox/9CM3Nzfj617+ONWvWFH3+85//HADw1a9+FT/84Q8LPksmk/jxj38MAPjyl7+M\nn/3sZwWfT5kyBY8//jgA4I477sAvf/nLgs9nzpyJ7373uwCAW2+9FT09PQWfv+td78KDDz4IALjl\nllvw6quvFnze1dWFBx54AABw0003Ydeuwroe559/Pu677z4AwDXXXIODBw8WfH7JJZfgb//2bwEA\nv/d7v4d0unA18YorrsBf/ZVWV4Vj78Qce6PZHG66+dPYsnU7hkdGMayLKNU+B+2X3gIAOPzjf0Di\nWP+YwGmM48IPLMGqz/09AOD8Sz6M11/fhZwpRnnSad144Gv34bL5pziOva/YjL0EgFYAf3z99fjj\nP/lTHBg4gms+ciWySiGbU8jlFLJK4XeuuA7nf+ga7H1nH75+558joxQO5hT265+f/sGPQhZehN7e\nXvR8917t73Iq38bJ7/0oms94HzIHd+Hgun8pOjetF3wMraefCzn0Nvau+3fEYkBcBPGYICaCD3zi\nf+H0d5+Hg2+8jGe++0+Ix4CY6fPb7vw7LOzuwqYX/wcP/tNXERdBTP/OgaPHMfzeTyM3eTqGX/8V\nNvz6B/jENwSnTWvJ1+wal2NP5YBMGu+aPhkP/ukSYN9W3PIv/4VX9xwZ++NYHF2nnYwH/nIZcNLZ\nuGnl49j1yvPA6Ej+K+fPjOO+Sycg1zwVH/7PFhw8sB+SWY9E7hdIYBSXnNqAv/2dJvwBGvCh72Uw\nSTUh3tSMhqYW9CWa0f2Rj+KyD/0NAN73TtT7HlD4zI0fPY7+Q2mo9tlYcO1nsXzpXDx2/+fxQKWf\nublRzYo2ov1cf3YCfzb/MIYzCh9+eBhoaAIaW4DGdqBxIm7+gz/EzZ+51TT2flqwfY69+hh79Yof\n0Wa3dGCNGX0SwCNKqeMi8qcAvg3g4qINidwC4BYA6OzsDNhUQgipb0azKi/M+t88iCv/+Rd49Z0j\n2L15DzKHjkBEkEzEMWlCAu+a044v/sF7cNb0ybh91yPYtaswZs3sjjgjlcSRaS3oPZTG8dEsmhri\n+N2zTy7bcpSIx9CaTNhavM7tbMPH39uJAwea8Z9txW5Mn7n4TNxwwxJt8vLfY+nDFTSXuVuuW4gL\nLv4dbPntNty9ebIu6oCs0oThped1YNaCU/Hm1iH86LkGXfBpLqKZbA6vvnMEb8bewYE33sHe/uGi\n/f/N2lfQ+MthpN/ejMGd/fn3RQRKKUzJZGHOzZZTCm/sH8Je3dX0j/7fS5jY3ou+Ddvx5u7DgBQ+\nDP/84d+geXIKbzz/Fl7beyS/yC7aTvC5NT1ompDE5g278Mb+o2Of6Rv6mx9sRkwEL27Zi7cPDkGM\nTwVINObwv3+yDTERvPD6AezqT5u2DQzmjuLf/vsNxATY1DuAPYPH8p8JgMz+o3jymefQduQ1HNz2\nPI7vfhsN2TTiWd2N9ngMuRc24ejk0zDcOBXplhSyDUlkG5qh4o3Y1fFuPDfvbxETwX71MxxMzEDb\n6A7ETI/+YdWIFQPX438OPIdsWmFCIo6JzQ2Y1CjYP/fdGLnyEjT2v47GtQ8iO3QEODIAHNH//plX\ngMaHtQQMh97U0m8nkloWvXhj0bkk45+pE5swdWITurpOxQMrtOnjY2HvJJvRLGbPfVWzoO16SUsc\nYtDQBEyeD1z8WSA1D3juvuIMjsniUgiEVAvP7JEicj6Au5VSS/XXdwCAUuo+h+/HARxSSrW6bZfZ\nIwkh45XRbA5vHhjCVpNr47Y9R7D38Fjs2ZSWRpw1fTLmnTJJ+z19Es44aSLrSpVALqeQzoy5gB49\nPlrkEmp2Bf3XZ99w3NZFc6chpzQRp/Tf2o+W1jyXf894rZDLFX9fmb7nta1szv1z58e0wik4hHmx\nXrxLejE3tgtzpRdnSB8miJZ5MacEO9RJ2K46sV3NxKu5WdimZmGHOhmjAUq1XhX7Bf66YQ1myEHs\nVlPwv0evx7ONF+KfPt6NrpkptLV4iK3REaD/La021YFXgf367wOvASMmS1/TZE3M6e6W+Z/2U91T\noBNi5vCeQvfGPZuAw31jn7efNpYcxHBxbG6vXXvJCU1oKf9FpAGay+Ml0LJDvgTgE0qpLabvTFdK\n7dH//1EAn1dKLXbbLkUbIWQ8cGhoBFv3HC4QaK/tO5qPB0vEBadPm4izpk/GWdMnYd4pmkA7aRJj\neWrFkpXP2CY+6Egl8fyKIieRmqOUghruR+6dLVpikH1bIft+i9j+rZDjh/Pfy008BaNTz8LolHnI\nTJ2HkSnzkGk7E9l40kOA2olM/f85hRsefNG2XQLgrZWXl3twwJE9YwLuwKtagocDrwFHdo99L9ag\nTbStYm7qmcCEyeW1gdQvSmllKKwC7eg7+hdEGyMFAu0cLQ6TkIgQWsp/pdSoiPwFgHXQUv5/Sym1\nRUS+BGC9UuoJAH8pIlcBGAVwCMDNZbWeEEIiRiabwxv7j2LbniPYqmdu3LbnMPYdGXOvmTapCfNO\nmYSbL5iTF2inT5tY0YQaJDiRS3xgZmQI2L+tICGI7NsKOboX+VE0oRU4aT6w4DrgpLOAk+cD0+Yh\n1tyORgBhOxh2VDK7nwgweYb2c9qFhZ8dOwwcfK1YzL36k7G06gAwabrJOjd37P+TZzA5RL3gpw6g\nUkD/28UCbViPa5MYMG0ecPrFYwLtlAVAU3GZAULqERbXJoQQC/uPHMe2vZpL49Y9h7F17xG8vu8I\nMlntftkYj+GMkyZi3vRJ+P/bu/P4Oqt63+OflaFJ2qbzSOdC6URLW9KCUJzgCDgwiCKCiIxOqNfh\nHFHP9ShXrxw993iOxxFbBhURZBIQRECGMqeF0gkKpWM6pnObNmmGdf94Upq2aZu2SZ6dnc/79cor\ne6/n2Xv/EnZ1f7PW81tj+nd5Z/asV1vZAFbpd4+srYYNi96ZOWPtguT2pqW8c9l4XmHyIbTPmPpw\nNia5Xdy/VcNIxm3eXlud/J7WN1hiWb4wud1g5pEOnRuEuRH1ge74ZMYuz2vnMsa++wBCco3je78D\nXfrvHdAqtyTHc/KSfxPvzKBNSP540aFjOj+DdBSabXlkSzG0SUrbrpo6Fq3bXr/n2e7ljdtYv33P\n7FnfLgWM6tdlr+WNw3t3It99ydQUdXWwZXn9zNmC+nD2ehIw6pLrzgi50PO45ENonzF7wln3oZCT\nGdc4ph5ym2L33nLrF+4T5t6CrQ26EIbc5Bq5fcNcrxE2mmgpMSahbFdFfafGinc6NnLvtXtmyxqT\n2yEJZA2XOPYdmzQOkbKAoU2S6sUYKd9WxetrkiWNuwPaonXb39nkt0NeDsf37czofl0Y1b8Lo/sV\nM6p/F3ocqsGCtNv28j17na2t/17+RvLBdLeug+vDWf2yxj6joecIyPcaxxZVtX3vpZa7m6FsfBtq\n92xnQOe+DcJcg2vnugyAnHbyh5q6OqjesU/AahCy9rrfyO2qfc+rP75f4/Em+OyM5N+ITWiUxZrt\nmjZJakuqamp5a+32PV0b65c5bqjY88Gsf9dCRvUr5v2j+jCqfxfG9C9maM9Oe7XRlw6ocmv9dWcL\n9g5oO9bvOadjz2S2bMKle2bOeo+yaUZaCjrDMROTr4Zqa2DzsgZLLevD3Lx79izFg2Q7gsa6WvY8\ntvEZn6Zco9Uc6mr3D0eHDFiHCFnVFU1//ZCb/G47dK7fz6xTcrvLMXvf3/d4w9t3XdagcUgDXQcl\nTUMkAc60Sa2iTSwtyjCH+p3FGFm7tYrXG1x79saarbxdXkFt/exZQV4OI/sV18+eFdcvcyymW0dn\nz9q9pnyorqlKPsTvu7Rxy/I95+R32n/mrM8Y6NyndX8eNa8YoWJ9fZBb2KAZypt7//cPOcky1oZL\nLbesgOd+BjX7XKP1of+CkWcfZKbqCGaxavZvEHNAuR0aCU2NhKiDBayGtws6J895tNdXHuiato/8\nrGWCrpRhXB4pZYjGLuIvyMvh8+89lveO7ENOgJwQCPXfk69kE+CcBmMhQE5OOLzzw97nhzbSSe1A\nv7MLJg2gKD+XN1Zv4401W9m0o/qd4wO6Fe3VUn90/y4M7dmJ3Jy28TOrFTX2ITGvEEqugKIee2bO\nNiyCWP8ezMlPPpg3bAjSZ3Sy3LG9LJtTYteO5L2x7+zchkVQW3XoxzdFXmHTglNTAlZB5+SPC5nc\nfKW1ZialDGRokzLEgfaESkPDIHfokNfweP35OYd5fmPPn9PY+Q1u58CTb6xjZ3Vdoz9DUX5uMnv2\nzsxZF0b2K6Zrkdc8tHsxJrNjVduSLoJV2/b52pp8PfvT5H6jQjJz8k5DkPqZs57HeV2NDq6uFjYv\nh59N5IDXb531o4OHrN0BK9erV6T2wmvapAxQVVN70MB2yxWTk01s62jyprd1dY1sgLvXYxse5/DP\n33cD3v1qO8T5jT1//XPU1kWqaw99/oECWwDmff8sZ8+yze7OcnuFq0bC1n4hrMG5lfXf66oP/XoH\nFODbK5MPz9LhyqnvStl1YLJEcl9dB8G7vtD6dUnKCoY2qYXMWraJ6++Zc8DjA7oV8b6RXvfSmAPN\nTh7TrcjAdiBpLC+Ksf46m0ZC1F4h6yCBa3fYirWHfr3cAigobvDVJfkgvNdY/XijY/XjPy85wIfq\ngQY2Hb0zvtv4NVpnfDe9miS1eYY2qZlVVNXwk0cXctsLS+nfpZBrTx/G719cvt/GtP981sj0isxw\n/3zWyEY38/V3dgD7XqO1ZUVyHxoPbnV1STODQ4atAwSu3UFr1zaIjc+K7iWvKAlLhQ3CVPehjQSr\nhoGryz5jnZtvXyY/VKsl7f435zVakpqR17RJzejpN8v59r1zWbVlJ5edMoR/OXsUnQvy7B55BPyd\nHYb/HLv35sG75RXBkHc1HraaokPnQwSrgwWtBl+ZeC2YjQ8kSRnARiRSK9pUsYv/89cF3PvKSo7t\n3Yl/v3A8JUN7pF2WslGMSbODstI9XytnHfj8gZMPvEzwoMsJi5NrdCRJUouxEYnUCmKMPDRnNd9/\ncD6bd1Rz3fuO47r3H0dhvh921Ux2VcCq2VD2MpTNhBUvQ8W65Fh+RzhmUhKwGuuG2HUQXP1469Yr\nSZKanaFNOkJrtlTyr/fP4/HX1zJ+YFd+f9XJjO7fJe2y1JbFCBsX75lBW/FysmfY7iYdPY6FY98P\ngyYnM2h9xiatwQ+0Oa3XaEmSlBUMbdJhqquL3FG6nBsffoPqujq+88HRXHHaUPJy3WBXh6lqW7K0\nsaw0mUUrK4UdG5JjHYphwCQ4/WtJQBtQAp16Nv48Nj6QJCmrGdqkw7C4fDvfuncuLy3ZyKnH9uRH\nHx3HkJ62CFcT1NXBhkX1yxxLYUUprFvAO5vw9hoJx5+zZxat96jDu6Zs/EWGNEmSspShTWqCmto6\nfjtjCT99/E0K8nL49wvHcVHJIEJwzzAdwM7NsHLmnuvQVs6Eyi3JscKuyczZmHNhYElyu6hbuvVK\nkqSMZWiTDmHeyi188545zF+1lbPH9uOG88bSp0th2mUpk9TVQvkbDa5FK4X1C+sPBugzBsZekMyg\nDZwMPUdAjstpJUlS0xjapAOorK7lv594i5ueWUz3jh341aWTOGdc/yN7MveEyi4VG+pn0eqbhax8\nZc/eZ0U9kmA2/uPJ92MmJZtKS5IkHSFDm9SIlxZv4Pp757JkfQUXlQzkOx8cQ9eOR7hB8L6d/bas\nSO6Dwa0tqK2BdfP3zKCVlcLGt5NjIRf6joUTP7FnFq3HcHDZrCRJakaGNqmBbZXV3PjIG9z+0nIG\n9SjiD1edzNQRvY7uSZ+4Ye9W7JDcf+irsGkpdO4DnfvVf+8LnXpDXoeje00due3r9syglc2EVa9A\n9Y7kWKfeMHAKTLqsfhZtInSwEY0kSWpZhjap3uML1vKv989j3bZKrp46jK994Hg6djiKfyIxwvIX\nkpm1xuzaDk/+sPFjRT2guEGQe+d7w68+UNTdWZ2jUbML1s7d0yykrBQ2L0uO5eRBv/Ew6dN7ZtG6\nDfb3LUmSWp2hTe3e+u1VfO+B+Tw0ZzWj+hXz68tOYsKgo+jkV1MF8+6BF38Fa+YAgXfaujfUdRB8\naRZUlMO2tbB999e6BrfXJsFv21qordr/OXLy9wS4RkPe7rE+yWbL7d3WVXvPoq2eDTWVybHiY5J2\n+1OuSQJa/xP9nUmSpIxgaFO7FWPkvldXcsNDC9hRVcvX/ul4PveeY+mQd4Rd/batgdLpMOuWJIj1\nHgUf/q8kWD3yjb2XSOYXJc1I8gqSxiRdBx6qWKjamgS6bWv2CXf13zcvTwJJxXoaDYkFXfeEueK+\n+4S73Us0+0LHntnR2bC6MgnNu2fQykph68rkWG4H6D8BJl+dtNwfOAW6Dki3XkmSpAMwtKldKtu0\ng2/fN49n3iznpCHdufGj4xjRt/jInmzlLHjx1zD/PqirgePPgpM/B8Pfu2cpXV6Ho+seGUKyt1dh\nV+g14uDn1tbAjvV7Al1jIW/V7OT2ru2NvFZucu3WfgGvkWWaBZ2b/jO0pBiTZai7Z9DKSpPAVrsr\nOd51MAw6GQZNSWbR+o1LArMkSVIbEGJs5C/yraCkpCTOnDkzlddW+1VbF/n9C0v58aPJHlrfPHsU\nl50yhJycw7xOqbYaXn8gCWtlL0OHYph4KUy5Fnoe2/yFt5Sq7VCxbk+ga3SZ5rrknLqa/R+f32nv\n2bq9lmg2CHmdekPuYXbfPNg2Cbt2JEsbG86ibV+bHMsrShqEDJq851q04n5H93uSJElqASGEWTHG\nkkOeZ2hTe/HW2m188545vLJ8M+85vjc/vOAEBnbveHhPUrEhWf5YOh22rUrau0/5LEy4JLv34qqr\ng50b9w90jYW8ys2NPEFIll02DHKNzuD1gcJuMPfPe2+TAMmSxsGnQeUmWDtvT4jsPmzPDNrAyUkL\n/sMNiJIkSSloamhzeaSy3q6aOn711Nv84slFdCrI5aefOJHzJwwgHE4XwLXzk8Yic/+cNK4Y/l74\n8E9hxAey4/qvQ8nJgU69kq++Yw9+bnXl3rN3ey3TrB/b8HbyvbHmKrkFSSCLtXuP1+6CJU/B0Klw\n2lf2hLROR7klgyRJUoYztCmrzV6xmW/ePYeFa7fxkROP4d8+MoZenZt4LVNdLbz5tySsLZ2RLLs7\n8ZNw8mehz+iWLbwtyy9MWuN3G3zw82KEyi2Nd8187r8P/LjPPNS89UqSJGU4Q5uy0o5dNfy/lGHS\nNwAAIABJREFUv7/JLc8toU9xIdMvL+GM0X2b9uCdm+HVP8DLNyV7dnUZCGd+P9mvq2OPli28PQkB\nirolX71H7n1s3r2N7293qC6bkiRJWcjQpqzz7Fvr+dZ9c1ixcSefOmUw3zx7FMWFTbjGaf1b8NJv\nYPYfoboCBp8K/3QDjPow5PpPpVWd8d39r2nbvU2CJElSO+MnUWWNLTuq+cFfF/DnWWUM79WJO689\nhZOH9zz4g+rq4O1/wEu/hkWPJc0uTvhYsgTymAmtU7j2t7tL5NFskyBJkpQlDG3KCo/MXc3//st8\nNu3YxRfeeyxfPmMEhfm5B35A1XZ47Y5kCeT6N5POhe/9NpRckXQwVPrGX2RIkyRJwtCmNm7d1kr+\n91/m8ej8tZwwoAu3XjGZEwZ0PfADNi1Lgtorv4eqLcl+XhfcBGMvSDbAliRJkjKMoU1tUoyRO0tX\n8MOHX2dXTR3XnzOKq6cOIy+3kfb7McLSZ5MlkAsfBgKMOQ9O+XzSMv5wWv9LkiRJrczQpjZn6foK\nvnXvXF5YvIGTh/XgxgvHM6xXp/1PrK5M9lV76Tewdi4U9YDT/hdMvhq6Dmj9wiVJkqQjYGhTm1FT\nW8fNzy3hPx97k/ycHP7vBeO4ePIgcnL2mSnbuhpKp8GsW2DHBugzFs79Hxj38aQDoSRJktSGGNrU\nJixYtZXr753DnLItnDm6Lz84/wT6dS3c+6QVpckSyAX3Jxtjj/wgnPI5GHq6SyAlSZLUZhnalNEq\nq2v5+T8W8eun36Zbx3x+fslEPjSuP2F3CKvZBQv+Ai/9ClbOgoIuMOWzMOUa6DEs3eIlSZKkZmBo\nU8YqXbqR6++Zw9vlFVw4aSD/+qHRdO9U3+FxeznMujVZBrl9DfQ8Dj74H3DiJ6Ggc6p1S5IkSc3J\n0KaMs72qhh//7Q1+98IyBnQr4rYrp/Ce43snB1fPSZZAzr0baqvg2DPgvJ8n33Ma6RwpSZIktXGG\nNmWUJ99Yx3fum8vqrZVccdpQvvGBkXTKAxY8kIS1Zc9BfkeYdFmyDLL38WmXLEmSJLUoQ5sywsaK\nXdzw4Hzun72KEX06c8/nT2VSb2DmL+DlabBlOXQbDB/4AUy8DIq6pV2yJEmS1CoMbUpVjJEHXlvF\n9x9cwLbKar5yxgi+OK6GDjNvgNf+BNU7YMhUOPv/Jt0gc3LTLlmSJElqVYY2pWbV5p185765PLmw\nnIkDu/DzKRUMWPhNeO4fkFsA4z+eLIHsPz7tUiVJkqTUGNrU6urqIre/tIwbH3mDwriTO8a/zinr\n7yY8/DYU94f3/yucdAV06pV2qZIkSVLqDG1qVYvWbedb985hzbI3+H89n+EDux4n581tMKAELpwO\nY86D3Py0y5QkSZIyhqFNraK6to6bnn6bl/5xP1/I+xvvLZgFO3IJYy+Akz8HA0vSLlGSJEnKSIY2\ntbi5S9fwj7v+h7O2/4Uv5q2grqgXYfI3oOQq6NI/7fIkSZKkjGZoU4upXL+MV+75D0avupevhO1s\n7T4K3vsLck74GOQXpl2eJEmS1CYY2tS8YoQVL7H+if+m27K/cXKMvN713eR/+Ot0GfFuCCHtCiVJ\nkqQ2xdCmIzPnLnjiBthSBl0Hwvu+DSGHmhd+Sd6a18iPHbm7w7kc/+GvMenEE9OuVpIkSWqzmhTa\nQghnA/8N5ALTYow3HuC8jwF/BibHGGc2W5XKLHPugge/DNU7k/tbVsD9nwdgOQO5ueZKup1yGded\ndSKF+W6GLUmSJB2NQ4a2EEIu8Avgn4AyoDSE8ECMccE+5xUDXwZeaolClUGeuGFPYGtgfezCdd1/\nxY8/fiInDOiaQmGSJElS9slpwjlTgEUxxsUxxl3An4DzGjnv/wA/BiqbsT5loLilrNHxnmEbf/nS\nVAObJEmS1IyaEtoGACsa3C+rH3tHCGEiMCjG+FAz1qYMtZZeBxzPz23KW0qSJElSUzXlE3Zj7f7i\nOwdDyAF+Cnz9kE8UwrUhhJkhhJnl5eVNr1IZ5Ue7Pk5N3PutsyN24Ee7Pp5SRZIkSVL2akpoKwMG\nNbg/EFjV4H4xcALwVAhhKXAK8EAIoWTfJ4ox3hRjLIkxlvTu3fvIq1aq5hdPpZpcKmIBdTFQVteL\n66uvZmaXf0q7NEmSJCnrNKV7ZCkwIoQwDFgJXAxcsvtgjHEL7FkvF0J4CviG3SOz13+MWkjRa9Vc\nUPV9Xo0jACjKz+VHZ41MuTJJkiQp+xxypi3GWANcBzwKvA7cFWOcH0K4IYRwbksXqAwTI8cvv5P5\ndUNYXDCaAAzoVsSPPjqO8ycOOOTDJUmSJB2eJu3TFmN8GHh4n7HvHuDc9x59WcpYK16m46Y3uDvn\nWp7/1hl0KnB/dkmSJKkl+Ylbh6XiuV9TF4sonnKJgU2SJElqBfZnV9NVrKfgzQe5t+7dXDJ1dNrV\nSJIkSe2CoU1NtvPl28iL1awecQn9uhamXY4kSZLULri+TU1TV0v1S9OYXTuGj5z5vrSrkSRJktoN\nZ9rUJDVvPkaXylW83PsCxh7TNe1yJEmSpHbDmTY1yfonf0lO7Mb4My859MmSJEmSmo0zbTqkuGkp\nfdY+w6MFH+A9o9yLTZIkSWpNhjYd0qonfkVdDHQ+9RpyckLa5UiSJEntiqFNB1dTRZcFd/BMKOGc\n005KuxpJkiSp3TG06aDWvnQXxXVb2Dj20xTm56ZdjiRJktTuGNp0UJXP/4alsR/vOetjaZciSZIk\ntUuGNh3Q1iWvMqRiLvP6X0jvLkVplyNJkiS1S4Y2HdCyv/8PlTGfUed8Lu1SJEmSpHbL0KZGVVVs\nYvjqv1La+X0cN2Rw2uVIkiRJ7ZahTY2a/8g0OlFJ8emfT7sUSZIkqV0ztGk/sa6OHgt+x5u5x3Hi\nye9LuxxJkiSpXTO0aT9zXvgbQ+uWs/WEywnBzbQlSZKkNBnatJ8dz/2GrXRi3NlXpF2KJEmS1O4Z\n2rSXRYsXc1LFDBYPOI+CouK0y5EkSZLaPUOb9vLW335Jh1DLsLO/lHYpkiRJkjC0qYF1WyoYv/Y+\n3u5cQtdBY9IuR5IkSRKGNjXw/CN3MCCsp/PpbqYtSZIkZQpDmwDYuauW3m/8nk25PelbckHa5UiS\nJEmqZ2gTAH9/9gXeFV+j4oRPQW5e2uVIkiRJqmdoE3V1kZ0vTCOGwIAzXBopSZIkZRJDm3h6wQo+\nsOsx1h5zJqHLMWmXI0mSJKkBQ5tY8Pht9Ajb6fP+L6ZdiiRJkqR9GNrauXkrt/CujfezueNQ8o59\nT9rlSJIkSdqHoa2de+SxR5mUs4jCd10DIaRdjiRJkqR9GNrasdVbdjLo7TuoDgUUlnwq7XIkSZIk\nNcLQ1o7d8cw8zs15jqrRF0JRt7TLkSRJktQIN+RqpyqqaqiadTsdQxVM/Wza5UiSJEk6AGfa2qm7\nSpfzsbq/U9HrRDhmQtrlSJIkSToAQ1s7VFsXeWXGQ4zIWUmnqW6mLUmSJGUyQ1s79Pf5azhrx0Ps\nyu8KYy9IuxxJkiRJB2Foa4fufnoWZ+fOJO+kyyC/KO1yJEmSJB2Eoa2deWX5Jsasvo88asmZfGXa\n5UiSJEk6BENbO3PLM4u4NP9Jaoa9D3oem3Y5kiRJkg7B0NaOrNi4g12v/5V+bCDv5GvSLkeSJElS\nExja2pFbnlvKZbmPU9v5GBhxVtrlSJIkSWoCQ1s7sWVnNS+UvsTUnLnkTr4Sct1XXZIkSWoLDG3t\nxJ2ly/lo3d+JIQ8mfTrtciRJkiQ1kdMt7UB1bR1/fHYhD+U/QxjzESjum3ZJkiRJkprImbZ24OG5\nqympeIrOcTtMvjrtciRJkiQdBkNblosxMm3GEq4u+Aex9ygYclraJUmSJEk6DIa2LPfyko2w6lVG\n1b1FKLkKQki7JEmSJEmHwdCW5X47YwlXFTxBzO8IJ34i7XIkSZIkHSZDWxZbXL6d0jfe5kM5zxPG\nXwSFXdMuSZIkSdJhMrRlsZufW8JFec+SX1cFJVelXY4kSZKkI2DL/yy1qWIXd89awYyiJ6HPFOg/\nPu2SJEmSJB0BZ9qy1O0vLWNS7Vx671phm39JkiSpDWtSaAshnB1CWBhCWBRCuL6R458LIcwNIcwO\nITwbQhjT/KWqqapqarnthWV8teszUNQDxpyXdkmSJEmSjtAhQ1sIIRf4BXAOMAb4ZCOh7I8xxnEx\nxgnAj4H/bPZK1WQPzF5FzrbVlFS+AJMug/zCtEuSJEmSdISaMtM2BVgUY1wcY9wF/AnYa+omxri1\nwd1OQGy+EnU4YoxMf3YJX+r6HMQ6OOmKtEuSJEmSdBSa0ohkALCiwf0y4OR9TwohfBH4GtABeH+z\nVKfD9uyi9Sxas4kLuz5BOO5M6DEs7ZIkSZIkHYWmzLSFRsb2m0mLMf4ixngs8E3gXxt9ohCuDSHM\nDCHMLC8vP7xK1STTZizho53mUFS1zgYkkiRJUhZoSmgrAwY1uD8QWHWQ8/8EnN/YgRjjTTHGkhhj\nSe/evZtepZrkzbXbePrNcr5U/DR0HQwj/intkiRJkiQdpaaEtlJgRAhhWAihA3Ax8EDDE0IIIxrc\n/RDwVvOVqKaaPmMJo/NXM2hzKZR8BnJy0y5JkiRJ0lE65DVtMcaaEMJ1wKNALnBzjHF+COEGYGaM\n8QHguhDCmUA1sAm4vCWL1v7Kt1Vx36srubX/i7AxHyZ+Ou2SJEmSJDWDpjQiIcb4MPDwPmPfbXD7\nK81clw7T719cRl7dDk7e+miyL1tnl59KkiRJ2aBJm2srs1VW1/KHF5fxLwPmk7trqw1IJEmSpCxi\naMsC976yko0VVXys7lHoMwYGn5J2SZIkSZKaiaGtjauri0x7djEf7bOWzhvnweSrIDS2S4MkSZKk\ntsjQ1sY99eY6FpdX8L+6PgMdOsP4T6RdkiRJkqRmZGhr4377zBJGdqlm0Oq/JYGtoDjtkiRJkiQ1\nI0NbGzZv5RZeWLyB7w2aTaipTJZGSpIkScoqhrY2bPqzS+jcITBl4/0w+F3Qd2zaJUmSJElqZoa2\nNmrNlkoefG0V3xy5htxNS2zzL0mSJGUpQ1sbdevzS6mLkQtr/wYde8Hoj6RdkiRJkqQWYGhrgyqq\navjjS8v45MgcOi55DCZ9GvIK0i5LkiRJUgvIS7sAHb4/z1zB1soaruv6AsQIJ30m7ZIkSZIktRBn\n2tqY2rrIzc8tZcqgTvR/+y44/izoPiTtsiRJkiS1EENbG/PYgjUs37iDbw1fDNvX2oBEkiRJynKG\ntjZm2owlDOpRxIQ1d0O3IXDsGWmXJEmSJKkFGdrakFeXb2Lmsk189cQ6wrLnoORKyPE/oSRJkpTN\n/MTfhkx7dgnFhXl8eNcjkNsBJn4q7ZIkSZIktTBDWxuxYuMOHpm7mstLetFh3l0w9gLo1CvtsiRJ\nkiS1MENbG3Hr80vJCYGru86Cqq1QclXaJUmSJElqBYa2NmBrZTV3lq7gw+P60W3+76DvOBg0Je2y\nJEmSJLUCQ1sbcOfLK9heVcOXRm6GNXNh8lUQQtplSZIkSWoFhrYMV11bxy3PLeGU4T04dumfoEMx\njPt42mVJkiRJaiWGtgz3yLw1rNpSyecnd4P598GET0JB57TLkiRJktRKDG0ZLMbItBmLGd6rE6dv\nfxRqdyV7s0mSJElqNwxtGezlJRuZU7aFK08bQs6sm2HIVOgzOu2yJEmSJLUiQ1sGm/bsErp3zOfj\n3RbC5mVJAxJJkiRJ7YqhLUMtWV/B46+v5VOnDKHg1VugUx8Y9eG0y5IkSZLUygxtGermZ5eQn5PD\n5WMCvPkonHQ55HVIuyxJkiRJrczQloE2Veziz7NWcN6EY+j1xh3JnmwnfSbtsiRJkiSlwNCWgf74\n8nIqq+u45tQB8Orv4fhzoOvAtMuSJEmSlAJDW4apqqnl1ueXcvqIXhy/8SmoKIfJtvmXJEmS2itD\nW4Z58LXVlG+r4prTh0PpdOg+DIa/P+2yJEmSJKXE0JZBdm+mPbJvMad3WQvLn0/a/Of4n0mSJElq\nr0wDGeS5RRt4Y802rjp9GGHmzZBbABMuTbssSZIkSSkytGWQ385YTK/OBZw3uhjm3AknXAgde6Rd\nliRJkqQUGdoyxJtrt/H0m+Vc/q4hFCy4G3ZtT5ZGSpIkSWrXDG0ZYvqMJRTm53DpyYOh9GbofyIM\nOCntsiRJkiSlzNCWAcq3VXHf7JVcOGkgPTa8Auvmw+Srk021JUmSJLVrhrYM8PsXl7Grpo4rpw6D\n0mlQ0DW5nk2SJElSu2doS1lldS1/eHEZZ47uw7FFO2HBX2DCJdChU9qlSZIkScoAhraU3fvKSjZW\n7OKqqcPh1d9BXTWUXJl2WZIkSZIyhKEtRXV1kenPLuaEAV04ZWhXmHkrDHs39D4+7dIkSZIkZQhD\nW4qeenMdb5dXcPXU4YRFj8OW5VBim39JkiRJexjaUjRtxhL6dSnkQ+P7Jw1IOveDUR9KuyxJkiRJ\nGcTQlpL5q7bw/Nsb+MxpQ8nfsgwWPQ4nfQZy89MuTZIkSVIGMbSlZPqMJXTskMsnpwyGWbdAyIGT\nLk+7LEmSJEkZxtCWgjVbKnngtVVcVDKIrnm18OofYNQHocsxaZcmSZIkKcMY2lJw2wtLqYuRK08b\nluzLtmODDUgkSZIkNcrQ1soqqmq4/cVlnDW2H4N7doSZ06HncTDsPWmXJkmSJCkDGdpa2d2zytha\nWcPVpw+H1XNgxUvJZto5/qeQJEmStD+TQiuqrYtMf3YJEwd346Qh3ZNZtrwimHBJ2qVJkiRJylCG\ntlb02IK1LN+4g2tOHw6VW2DOn2HchVDUPe3SJEmSJGWoJoW2EMLZIYSFIYRFIYTrGzn+tRDCghDC\nnBDCEyGEIc1fats3bcZiBnYv4gNj+sJrd0J1hQ1IJEmSJB3UIUNbCCEX+AVwDjAG+GQIYcw+p70K\nlMQYxwN3Az9u7kLbuleXb2Lmsk1cedow8nJCsjTymEkwYFLapUmSJEnKYE2ZaZsCLIoxLo4x7gL+\nBJzX8IQY45Mxxh31d18EBjZvmW3ftGeXUFyYx0WTB8Gy56D8DZjsLJskSZKkg2tKaBsArGhwv6x+\n7ECuAh5p7EAI4doQwswQwszy8vKmV9nGrdi4g0fmruaSKYPpXJAHpdOgsBuM/WjapUmSJEnKcE0J\nbaGRsdjoiSF8CigBftLY8RjjTTHGkhhjSe/evZteZRt36/NLyQmBy08dCtvWwusPwsRPQYeOaZcm\nSZIkKcM1JbSVAYMa3B8IrNr3pBDCmcB3gHNjjFXNU17bt7WymjtLV/Ch8f05plsRvPI7qKtJ9maT\nJEmSpENoSmgrBUaEEIaFEDoAFwMPNDwhhDAR+A1JYFvX/GW2XXe+vILtVTVcPXU41NbArFth+Pug\n57FplyZJkiSpDThkaIsx1gDXAY8CrwN3xRjnhxBuCCGcW3/aT4DOwJ9DCLNDCA8c4OnalZraOm55\nbgknD+vBuIFd4a1HYWuZDUgkSZIkNVleU06KMT4MPLzP2Hcb3D6zmevKCg/PW8OqLZV8/7wTkoHS\n6VB8DBx/TrqFSZIkSWozmrS5tg5fjJFpMxYzrFcnzhjVBza8DW8/ASd9BnKblJUlSZIkydDWUkqX\nbmJO2RaunDqMnJwAM2+GnDyY9Om0S5MkSZLUhhjaWsi0GYvp1jGfj00aCNU7YfbtMOrD0KV/2qVJ\nkiRJakMMbS1gyfoKHnt9LZ86eQhFHXJh/n2wc5MNSCRJkiQdNkNbC7jluSXk5+Tw6VOHJAOl06HX\n8TD09HQLkyRJktTmGNqa2eYdu/jzzDLOnXAMfYoLYdWrsHImlFwFIaRdniRJkqQ2xtDWzG5/aTk7\nq2u5+vRhyUDpdMjvCCdenG5hkiRJktokQ1sz2lVTx23PL+X0Eb0Y1a8L7NwMc++GcR+Hom5plydJ\nkiSpDTK0NaMHX1vFum1VXH368GTgtTugZqcNSCRJkiQdMUNbM4kx8tsZizm+b2fePaIXxJgsjRw4\nGfqfmHZ5kiRJktooQ1szef7tDbyxZhtXTx1OCAGWPAMb3koakEiSJEnSETK0NZPfzlhMr84dOHfC\nMcnAzOlQ1B3GXpBuYZIkSZLaNENbM3hr7TaeWljOp981lML8XNi6Gl5/CCZ+CvIL0y5PkiRJUhtm\naGsG059dQkFeDpeePDgZeOU2iLVQcmW6hUmSJElq8wxtR2n99irufXUlF540kJ6dC6C2GmbdCsed\nCT2Gp12eJEmSpDbO0HaUfv/CMnbV1HHlafWbaS98BLattgGJJEmSpGZhaDsKldW1/OHFZZwxqg/H\n9emcDM6cDl0HwfFnpVucJEmSpKxgaDsK9726kg0Vu7jq9PpZtvVvweKn4KTLISc31dokSZIkZQdD\n2xGqq4tMf3YJY4/pwruG90wGZ94MOfkw8dPpFidJkiQpaxjajtDTb5azaN12rj59WLKZ9q4dMPt2\nGHMuFPdNuzxJkiRJWcLQdoSmPbuYfl0K+dC4+s20590DlVtsQCJJkiSpWRnajsD8VVt4btEGLj91\nKB3y6n+FM6dD79Ew5NR0i5MkSZKUVQxtR2D6s0vo2CGXS6bUb6a9chasehUmXwUhpFucJEmSpKxi\naDtMa7dW8uBrq7ioZBBdO+Yng6U3Q34nGP+JdIuTJEmSlHUMbYfptueXUlMX92ymvWMjzLsbxl8E\nhV3SLU6SJElS1jG0HYYdu2q4/aXlnDWmH4N7dkwGZ/8RaiqTpZGSJEmS1MwMbYfh7lllbNlZzTXv\nrp9lq6tL9mYbdAr0G5ducZIkSZKykqGtiWrrN9OeMKgbkwZ3TwaXPAUb33aWTZIkSVKLMbQ10WML\n1rJsww6uOX14spk2QOl06NgTxpyXbnGSJEmSspahrYmmP7uYAd2KOGts32Rgy0pY+DBMvAzyCtIt\nTpIkSVLWMrQ1wewVmylduokrpw4jL7f+VzbrVogRSq5ItTZJkiRJ2c3Q1gTTZiymuCCPi0oGJgO1\n1fDKbTDiA9B9aKq1SZIkScpuhrZDKNu0g0fmreGTJw+muLB+M+03HoLta21AIkmSJKnF5aVdQKa7\n9bmlAFx+6tA9g6XTodtgOO7MVGqSJEmSskF1dTVlZWVUVlamXUqLKiwsZODAgeTn5x/R4w1tB7G1\nspo/la7gQ+P6M6BbUTJYvhCWzoAz/g1yctMtUJIkSWrDysrKKC4uZujQoXs6tGeZGCMbNmygrKyM\nYcOGHdFzuDzyIO4qXcH2qhquPr3BL7d0OuR2SLpGSpIkSTpilZWV9OzZM2sDG0AIgZ49ex7VbKKh\n7QBqauu45bmlTBnWg/EDuyWDuyrgtTuSfdk69063QEmSJCkLZHNg2+1of0ZD2wE8Mm8NKzfv5Oqp\nDWbZ5v4ZqrbC5KvTK0ySJElSu2Joa0SMkWkzFjO0Z0fOHN139yCUToO+J8Cgk9MtUJIkSWqH7n91\nJafd+A+GXf9XTrvxH9z/6sqjer7Nmzfzy1/+8rAf98EPfpDNmzcf1WsfDkNbI2Yu28RrZVu4auow\ncnLqpzLLZsKauVByJbSDKVxJkiQpk9z/6kq+de9cVm7eSQRWbt7Jt+6de1TB7UChrba29qCPe/jh\nh+nWrdsRv+7hsntkI377zGK6dcznwpMG7hmcOR06FMP4i9IrTJIkScpS339wPgtWbT3g8VeXb2ZX\nbd1eYzura/mXu+dwx8vLG33MmGO68G8fGXvA57z++ut5++23mTBhAvn5+XTu3Jn+/fsze/ZsFixY\nwPnnn8+KFSuorKzkK1/5Ctdeey0AQ4cOZebMmWzfvp1zzjmHqVOn8vzzzzNgwAD+8pe/UFRUdAS/\ngQNzpm0fS9dX8Njra7n05MF07FCfaSs2wLx74cRPQEFxugVKkiRJ7dC+ge1Q401x4403cuyxxzJ7\n9mx+8pOf8PLLL/PDH/6QBQsWAHDzzTcza9YsZs6cyc9+9jM2bNiw33O89dZbfPGLX2T+/Pl069aN\ne+6554jrORBn2urd/+pKfvLoQlZu3glA3+KCPQdn/wFqq6DkqpSqkyRJkrLbwWbEAE678R/vfFZv\naEC3Iu787LuapYYpU6bstZfaz372M+677z4AVqxYwVtvvUXPnj33esywYcOYMGECACeddBJLly5t\nlloacqaNvdfH7vajRxYm62Pr6mDmzTDkNOg7JsUqJUmSpPbrn88aSVF+7l5jRfm5/PNZI5vtNTp1\n6vTO7aeeeorHH3+cF154gddee42JEyc2utdaQcGeyZ7c3FxqamqarZ7dDG3ATx5dyM7qvS823Fld\ny08eXQhv/wM2LU0akEiSJElKxfkTB/Cjj45jQLciAskM248+Oo7zJw444ucsLi5m27ZtjR7bsmUL\n3bt3p2PHjrzxxhu8+OKLR/w6R8vlkcCqRqZZ3xmf+Qfo1BtGn9vKVUmSJElq6PyJA44qpO2rZ8+e\nnHbaaZxwwgkUFRXRt2/fd46dffbZ/PrXv2b8+PGMHDmSU045pdle93AZ2oBjuhU1uj52Ytdt8Obf\nYOpXIa9DCpVJkiRJakl//OMfGx0vKCjgkUceafTY7uvWevXqxbx5894Z/8Y3vtHs9YHLI4EDr4/9\n0eBZyZ2TPtP6RUmSJEkShjag8fWxN54/kpGr7oMRZ0G3wWmXKEmSJKmdcnlkvf3Wx869GyrKYfLV\n6RUlSZIkqd1zpu1AZt4M3YfCse9PuxJJkiRJ7ViTQlsI4ewQwsIQwqIQwvWNHH93COGVEEJNCOFj\nzV9mK1u7AJY9l7T5zzHXSpIkSUrPIRNJCCEX+AVwDjAG+GQIYd9dppcDnwEab73S1sy8GXILYMKn\n0q5EkiRJUjvXlGmkKcCiGOPiGOMu4E/AeQ1PiDEujTHOAepaoMbWVbUNXvsTjL0AOvVMuxpJkiRJ\nu825C356AnyvW/J9zl2t+vKdO3du1dfbrSmNSAYAKxrcLwNOPpIXCyFcC1wLMHhwhnU2tA6lAAAI\nmElEQVRknHMXPHEDbKn/UbsPS7ceSZIkSXvMuQse/DJU1++vvGVFch9g/EXp1dUKmhLaQiNj8Uhe\nLMZ4E3ATQElJyRE9R4vY9w0A8Px/Qc/hWf8GkCRJkjLCI9fDmrkHPl5WCrVVe49V74S/XAezbmv8\nMf3GwTk3HvApv/nNbzJkyBC+8IUvAPC9732PEALPPPMMmzZtorq6mh/84Aecd955B3yO1tCU5ZFl\nwKAG9wcCq1qmnJQ8ccPegQ2S+0/ckE49kiRJkva2b2A71HgTXHzxxdx5553v3L/rrru44ooruO++\n+3jllVd48skn+frXv06M6c43NWWmrRQYEUIYBqwELgYuadGqWtuWssMblyRJktS8DjIjBiTXsG1Z\nsf9410FwxV+P6CUnTpzIunXrWLVqFeXl5XTv3p3+/fvz1a9+lWeeeYacnBxWrlzJ2rVr6dev3xG9\nRnM45ExbjLEGuA54FHgduCvGOD+EcEMI4VyAEMLkEEIZ8HHgNyGE+S1ZdLPrOvDwxiVJkiS1rjO+\nC/lFe4/lFyXjR+FjH/sYd999N3feeScXX3wxt99+O+Xl5cyaNYvZs2fTt29fKisrj+o1jlZTZtqI\nMT4MPLzP2Hcb3C4lWTbZNp3x3f2vaWuGN4AkSZKkZrK718QTNyQr4roOTD6vH2UPiosvvphrrrmG\n9evX8/TTT3PXXXfRp08f8vPzefLJJ1m2bFkzFH90mhTasl4LvQEkSZIkNaPxFzX7Z/SxY8eybds2\nBgwYQP/+/bn00kv5yEc+QklJCRMmTGDUqFHN+npHwtC2Wwu8ASRJkiRlvrlz93St7NWrFy+88EKj\n523fvr21StpLU7pHSpIkSZJSYmiTJEmSpAxmaJMkSZKUmrT3QGsNR/szGtokSZIkpaKwsJANGzZk\ndXCLMbJhwwYKCwuP+DlsRCJJkiQpFQMHDqSsrIzy8vK0S2lRhYWFDBx45DukGdokSZIkpSI/P59h\nw4alXUbGc3mkJEmSJGUwQ5skSZIkZTBDmyRJkiRlsJBWp5YQQjmwLJUXP7hewPq0i1DW8v2lluZ7\nTC3J95daku8vtaRMfX8NiTH2PtRJqYW2TBVCmBljLEm7DmUn319qab7H1JJ8f6kl+f5SS2rr7y+X\nR0qSJElSBjO0SZIkSVIGM7Tt76a0C1BW8/2lluZ7TC3J95daku8vtaQ2/f7ymjZJkiRJymDOtEmS\nJElSBjO0SZIkSVIGM7Q1EEI4O4SwMISwKIRwfdr1KHuEEAaFEJ4MIbweQpgfQvhK2jUp+4QQckMI\nr4YQHkq7FmWXEEK3EMLdIYQ36v937F1p16TsEUL4av3/N84LIdwRQihMuya1bSGEm0MI60II8xqM\n9QghPBZCeKv+e/c0azxchrZ6IYRc4BfAOcAY4JMhhDHpVqUsUgN8PcY4GjgF+KLvL7WArwCvp12E\nstJ/A3+LMY4CTsT3mZpJCGEA8GWgJMZ4ApALXJxuVcoCtwJn7zN2PfBEjHEE8ET9/TbD0LbHFGBR\njHFxjHEX8CfgvJRrUpaIMa6OMb5Sf3sbyQeeAelWpWwSQhgIfAiYlnYtyi4hhC7Au4HpADHGXTHG\nzelWpSyTBxSFEPKAjsCqlOtRGxdjfAbYuM/wecBt9bdvA85v1aKOkqFtjwHAigb3y/BDtVpACGEo\nMBF4Kd1KlGX+C/gXoC7tQpR1hgPlwC31y2+nhRA6pV2UskOMcSXwH8ByYDWwJcb493SrUpbqG2Nc\nDckf04E+KddzWAxte4RGxtwPQc0qhNAZuAf4XzHGrWnXo+wQQvgwsC7GOCvtWpSV8oBJwK9ijBOB\nCtrYsiJlrvrris4DhgHHAJ1CCJ9Ktyop8xja9igDBjW4PxCn59WMQgj5JIHt9hjjvWnXo6xyGnBu\nCGEpydLu94cQ/pBuScoiZUBZjHH36oC7SUKc1BzOBJbEGMtjjNXAvcCpKdek7LQ2hNAfoP77upTr\nOSyGtj1KgREhhGEhhA4kF8E+kHJNyhIhhEByPcjrMcb/TLseZZcY47dijANjjENJ/rfrHzFG/1Kt\nZhFjXAOsCCGMrB86A1iQYknKLsuBU0IIHev/v/IMbHSjlvEAcHn97cuBv6RYy2HLS7uATBFjrAkh\nXAc8StK56OYY4/yUy1L2OA24DJgbQphdP/btGOPDKdYkSU31JeD2+j9qLgauSLkeZYkY40shhLuB\nV0g6Lb8K3JRuVWrrQgh3AO8FeoUQyoB/A24E7gohXEXyx4KPp1fh4QsxetmWJEmSJGUql0dKkiRJ\nUgYztEmSJElSBjO0SZIkSVIGM7RJkiRJUgYztEmSJElSBjO0SZLavBBCbQhhdoOv65vxuYeGEOY1\n1/NJknS43KdNkpQNdsYYJ6RdhCRJLcGZNklS1gohLA0h/HsI4eX6r+Pqx4eEEJ4IIcyp/z64frxv\nCOG+EMJr9V+n1j9VbgjhtyGE+SGEv4cQilL7oSRJ7Y6hTZKUDYr2WR75iQbHtsYYpwA/B/6rfuzn\nwO9ijOOB24Gf1Y//DHg6xngiMAmYXz8+AvhFjHEssBm4sIV/HkmS3hFijGnXIEnSUQkhbI8xdm5k\nfCnw/hjj4hBCPrAmxtgzhLAe6B9jrK4fXx1j7BVCKAcGxhirGjzHUOCxGOOI+vvfBPJjjD9o+Z9M\nkiRn2iRJ2S8e4PaBzmlMVYPbtXhNuCSpFRnaJEnZ7hMNvr9Qf/t54OL625cCz9bffgL4PEAIITeE\n0KW1ipQk6UD8S6EkKRsUhRBmN7j/txjj7rb/BSGEl0j+UPnJ+rEvAzeHEP4ZKAeuqB//CnBTCOEq\nkhm1zwOrW7x6SZIOwmvaJElZq/6atpIY4/q0a5Ek6Ui5PFKSJEmSMpgzbZIkSZKUwZxpkyRJkqQM\nZmiTJEmSpAxmaJMkSZKkDGZokyRJkqQMZmiTJEmSpAz2/wEhB/Y5L4YUjAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf99613748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer network\n",
    "Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n",
    "\n",
    "Read through the `FullyConnectedNet` class in the file `cs231n/classifiers/fc_net.py`.\n",
    "\n",
    "Implement the initialization, the forward pass, and the backward pass. For the moment don't worry about implementing dropout or batch/layer normalization; we will add those features soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial loss and gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-7 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "Initial loss:  3.3919674304905376\n",
      "W1 relative error: 5.65e-07\n",
      "W2 relative error: 4.67e-07\n",
      "b1 relative error: 7.92e-08\n",
      "b2 relative error: 5.19e-10\n",
      "Running check with reg =  3.14\n",
      "Initial loss:  6.926468493472546\n",
      "W1 relative error: 2.15e-08\n",
      "W2 relative error: 3.17e-08\n",
      "b1 relative error: 5.74e-09\n",
      "b2 relative error: 1.09e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "  \n",
    "  # Most of the errors should be on the order of e-7 or smaller.   \n",
    "  # NOTE: It is fine however to see an error for W2 on the order of e-5\n",
    "  # for the check when reg = 0.0\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another sanity check, make sure you can overfit a small dataset of 50 images. First we will try a three-layer network with 100 units in each hidden layer. In the following cell, tweak the learning rate and initialization scale to overfit and achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: 4.603684\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'argmax'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-eb3e7730cc9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                 }\n\u001b[1;32m     22\u001b[0m          )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conv-neural-nets/assignment2_v2/spring1718_assignment2_v2/cs231n/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst_it\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlast_it\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 train_acc = self.check_accuracy(self.X_train, self.y_train,\n\u001b[0;32m--> 285\u001b[0;31m                     num_samples=self.num_train_samples)\n\u001b[0m\u001b[1;32m    286\u001b[0m                 val_acc = self.check_accuracy(self.X_val, self.y_val,\n\u001b[1;32m    287\u001b[0m                     num_samples=self.num_val_samples)\n",
      "\u001b[0;32m~/conv-neural-nets/assignment2_v2/spring1718_assignment2_v2/cs231n/solver.py\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[0;34m(self, X, y, num_samples, batch_size)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \"\"\"\n\u001b[0;32m-> 1004\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# TODO: Use a three-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-3\n",
    "learning_rate = 1e-3\n",
    "model = FullyConnectedNet([100, 100],\n",
    "              weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again you will have to adjust the learning rate and weight initialization, but you should be able to achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: 4.605170\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'argmax'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-07704129b0de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                 }\n\u001b[1;32m     22\u001b[0m          )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conv-neural-nets/assignment2_v2/spring1718_assignment2_v2/cs231n/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst_it\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlast_it\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 train_acc = self.check_accuracy(self.X_train, self.y_train,\n\u001b[0;32m--> 285\u001b[0;31m                     num_samples=self.num_train_samples)\n\u001b[0m\u001b[1;32m    286\u001b[0m                 val_acc = self.check_accuracy(self.X_val, self.y_val,\n\u001b[1;32m    287\u001b[0m                     num_samples=self.num_val_samples)\n",
      "\u001b[0;32m~/conv-neural-nets/assignment2_v2/spring1718_assignment2_v2/cs231n/solver.py\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[0;34m(self, X, y, num_samples, batch_size)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \"\"\"\n\u001b[0;32m-> 1004\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# TODO: Use a five-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "learning_rate = 2e-3\n",
    "weight_scale = 1e-5\n",
    "model = FullyConnectedNet([100, 100, 100, 100],\n",
    "                weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 2: \n",
    "Did you notice anything about the comparative difficulty of training the three-layer net vs training the five layer net? In particular, based on your experience, which network seemed more sensitive to the initialization scale? Why do you think that is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "[FILL THIS IN]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochastic gradient descent. See the Momentum Update section at http://cs231n.github.io/neural-networks-3/#sgd for more information.\n",
    "\n",
    "Open the file `cs231n/optim.py` and read the documentation at the top of the file to make sure you understand the API. Implement the SGD+momentum update rule in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  8.882347033505819e-09\n",
      "velocity error:  4.269287743278663e-09\n"
     ]
    }
   ],
   "source": [
    "from cs231n.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "# Should see relative errors around e-8 or less\n",
    "print('next_w error: ', rel_error(next_w, expected_next_w))\n",
    "print('velocity error: ', rel_error(expected_velocity, config['velocity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with  sgd\n",
      "(Iteration 1 / 200) loss: 8.233277\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'argmax'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1d3d58b0a337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                   verbose=True)\n\u001b[1;32m     22\u001b[0m   \u001b[0msolvers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_rule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conv-neural-nets/assignment2_v2/spring1718_assignment2_v2/cs231n/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst_it\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlast_it\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 train_acc = self.check_accuracy(self.X_train, self.y_train,\n\u001b[0;32m--> 285\u001b[0;31m                     num_samples=self.num_train_samples)\n\u001b[0m\u001b[1;32m    286\u001b[0m                 val_acc = self.check_accuracy(self.X_val, self.y_val,\n\u001b[1;32m    287\u001b[0m                     num_samples=self.num_val_samples)\n",
      "\u001b[0;32m~/conv-neural-nets/assignment2_v2/spring1718_assignment2_v2/cs231n/solver.py\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[0;34m(self, X, y, num_samples, batch_size)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \"\"\"\n\u001b[0;32m-> 1004\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-2,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp and Adam\n",
    "RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "In the file `cs231n/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check your implementations using the tests below.\n",
    "\n",
    "**NOTE:** Please implement the _complete_ Adam update rule (with the bias correction mechanism), not the first simplified version mentioned in the course notes. \n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  9.524687511038133e-08\n",
      "cache error:  2.6477955807156126e-09\n"
     ]
    }
   ],
   "source": [
    "# Test RMSProp implementation\n",
    "from cs231n.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('cache error: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  1.1395691798535431e-07\n",
      "v error:  4.208314038113071e-09\n",
      "m error:  4.214963193114416e-09\n"
     ]
    }
   ],
   "source": [
    "# Test Adam implementation\n",
    "from cs231n.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('v error: ', rel_error(expected_v, config['v']))\n",
    "print('m error: ', rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with  adam\n",
      "(Iteration 1 / 200) loss: 9.457637\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'argmax'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8f0107224199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                   verbose=True)\n\u001b[1;32m     13\u001b[0m   \u001b[0msolvers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_rule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conv-neural-nets/assignment2_v2/spring1718_assignment2_v2/cs231n/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst_it\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlast_it\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 train_acc = self.check_accuracy(self.X_train, self.y_train,\n\u001b[0;32m--> 285\u001b[0;31m                     num_samples=self.num_train_samples)\n\u001b[0m\u001b[1;32m    286\u001b[0m                 val_acc = self.check_accuracy(self.X_val, self.y_val,\n\u001b[1;32m    287\u001b[0m                     num_samples=self.num_val_samples)\n",
      "\u001b[0;32m~/conv-neural-nets/assignment2_v2/spring1718_assignment2_v2/cs231n/solver.py\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[0;34m(self, X, y, num_samples, batch_size)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \"\"\"\n\u001b[0;32m-> 1004\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rates[update_rule]\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 3:\n",
    "\n",
    "AdaGrad, like Adam, is a per-parameter optimization method that uses the following update rule:\n",
    "\n",
    "```\n",
    "cache += dw**2\n",
    "w += - learning_rate * dw / (np.sqrt(cache) + eps)\n",
    "```\n",
    "\n",
    "John notices that when he was training a network with AdaGrad that the updates became very small, and that his network was learning slowly. Using your knowledge of the AdaGrad update rule, why do you think the updates would become very small? Would Adam have the same issue?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a good model!\n",
    "Train the best fully-connected model that you can on CIFAR-10, storing your best model in the `best_model` variable. We require you to get at least 50% accuracy on the validation set using a fully-connected net.\n",
    "\n",
    "If you are careful it should be possible to get accuracies above 55%, but we don't require it for this part and won't assign extra credit for doing so. Later in the assignment we will ask you to train the best convolutional network that you can on CIFAR-10, and we would prefer that you spend your effort working on convolutional nets rather than fully-connected nets.\n",
    "\n",
    "You might find it useful to complete the `BatchNormalization.ipynb` and `Dropout.ipynb` notebooks before completing this part, since those techniques can help you train powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# find batch/layer normalization and dropout useful. Store your best model in  #\n",
    "# the best_model variable.                                                     #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your model!\n",
    "Run your best model on the validation and test sets. You should achieve above 50% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
